{"cells":[{"cell_type":"code","execution_count":1,"id":"5878267e","metadata":{"deletable":false,"editable":false,"jupyter":{"outputs_hidden":true,"source_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"5878267e","executionInfo":{"status":"ok","timestamp":1681734543098,"user_tz":-180,"elapsed":1786,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"348e3cf4-192f-4be6-a217-e7e54dd8fb88"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Completed with errors. Exit status: 256\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["256"]},"metadata":{},"execution_count":1}],"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2023-spring/lab1-1.git .tmp\n"," mv .tmp/tests ./\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"]},{"cell_type":"code","source":["!gdown --id 1QTpo-Op_UFiAVV3zwlxqXcyyCAxK1gga"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3M9Bb6LMcf1","executionInfo":{"status":"ok","timestamp":1681734661512,"user_tz":-180,"elapsed":1304,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"b85eab96-9454-4bb1-fa8e-514de3c33bb7"},"id":"-3M9Bb6LMcf1","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1QTpo-Op_UFiAVV3zwlxqXcyyCAxK1gga\n","To: /content/requirements.txt\n","100% 37.0/37.0 [00:00<00:00, 50.6kB/s]\n"]}]},{"cell_type":"code","source":["!pip install -r /content/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ioYaxP-4MOEb","executionInfo":{"status":"ok","timestamp":1681734679449,"user_tz":-180,"elapsed":8448,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"94b7422a-80f7-4232-8eb3-dc439528ce64"},"id":"ioYaxP-4MOEb","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from -r /content/requirements.txt (line 1)) (3.8.1)\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from -r /content/requirements.txt (line 2)) (2.0.0+cu118)\n","Collecting otter-grader==1.0.0\n","  Downloading otter_grader-1.0.0-py3-none-any.whl (163 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.0/164.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (5.8.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.5.3)\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker\n","  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.65.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (3.1.2)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.5.4)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (7.34.0)\n","Collecting dill\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pdfkit\n","  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (67.6.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->-r /content/requirements.txt (line 1)) (8.1.3)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->-r /content/requirements.txt (line 1)) (2022.10.31)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->-r /content/requirements.txt (line 1)) (1.2.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->-r /content/requirements.txt (line 2)) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->-r /content/requirements.txt (line 2)) (2.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->-r /content/requirements.txt (line 2)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->-r /content/requirements.txt (line 2)) (1.11.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->-r /content/requirements.txt (line 2)) (3.11.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.9.0->-r /content/requirements.txt (line 2)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.9.0->-r /content/requirements.txt (line 2)) (16.0.1)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.9/dist-packages (from docker->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.26.15)\n","Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.9/dist-packages (from docker->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (23.0)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from docker->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.27.1)\n","Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.5.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (5.7.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.14.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.7.5)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.1.6)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.8.0)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (3.0.38)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.4.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.1.2)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.2.2)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (5.3.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.5.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.9.2)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.8.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.7.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.2.1)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.7.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.0.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.11.2)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.16.3)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.3.3)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.22.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.9.0->-r /content/requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.8.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (22.2.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.19.3)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (3.2.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.1.12)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.2.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.16.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->docker->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->docker->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->docker->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (3.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.5.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (23.2.1)\n","Installing collected packages: pdfkit, PyPDF2, jedi, dill, docker, otter-grader\n","Successfully installed PyPDF2-3.0.1 dill-0.3.6 docker-6.0.1 jedi-0.18.2 otter-grader-1.0.0 pdfkit-1.0.0\n"]}]},{"cell_type":"code","source":["!gdown --id 1I3EzoMojOx_QQe2uAMk-i4YLUd1rU4DB"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uUftocBsNd1A","executionInfo":{"status":"ok","timestamp":1681735112019,"user_tz":-180,"elapsed":1911,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"8e07db3c-b001-4eaf-aeab-870d6dc5b790"},"id":"uUftocBsNd1A","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1I3EzoMojOx_QQe2uAMk-i4YLUd1rU4DB\n","To: /content/tests.zip\n","100% 5.38k/5.38k [00:00<00:00, 6.76MB/s]\n"]}]},{"cell_type":"code","source":["!unzip /content/tests.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHW1CL6FOWBO","executionInfo":{"status":"ok","timestamp":1681735127958,"user_tz":-180,"elapsed":1135,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"daee5d89-1756-48df-9b94-80509c558458"},"id":"pHW1CL6FOWBO","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/tests.zip\n","  inflating: tests/anywhere_1hot.py  \n","  inflating: tests/anywhere_1hot_reverse.py  \n","  inflating: tests/cosine_distance.py  \n","  inflating: tests/euclidean_distance.py  \n","  inflating: tests/example_bow.py    \n","  inflating: tests/example_sow.py    \n","  inflating: tests/hamming_distance.py  \n","  inflating: tests/jaccard_distance.py  \n","  inflating: tests/nltk_norm_tokens_punc.py  \n","  inflating: tests/nltk_whitespace_tokenize_and_nltk_normpunc_tokenize.py  \n","  inflating: tests/norm_tokens_punc.py  \n","  inflating: tests/normalize_token.py  \n","  inflating: tests/token_count.py    \n","  inflating: tests/token_count_punc.py  \n","  inflating: tests/token_count_whitespace.py  \n","  inflating: tests/tokens_whitespace.py  \n","  inflating: tests/type_count.py     \n","  inflating: tests/type_count_norm_punc.py  \n"]}]},{"cell_type":"code","execution_count":6,"id":"cf6948a5","metadata":{"deletable":false,"editable":false,"id":"cf6948a5","executionInfo":{"status":"ok","timestamp":1681734692524,"user_tz":-180,"elapsed":2292,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"]},{"cell_type":"markdown","id":"382f17fa","metadata":{"tags":["remove_for_latex"],"id":"382f17fa"},"source":["# Course 236299\n","## Lab 1-1 – Types, tokens, and representing text"]},{"cell_type":"code","execution_count":7,"id":"682e20f5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"682e20f5","executionInfo":{"status":"ok","timestamp":1681734704502,"user_tz":-180,"elapsed":7677,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"9e6644e2-29cf-4e4a-f11e-2587f37953fd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["import math\n","import re\n","import sys\n","\n","import torch\n","import nltk\n","\n","nltk.download('punkt', quiet=True) # this module is used to tokenize the text"]},{"cell_type":"markdown","id":"e3af2f9c","metadata":{"id":"e3af2f9c"},"source":["Where we're headed: Nearest neighbor text classification works by classifying a novel text with the same class as that of the training text that is closest according to some distance metric. These metrics are calculated based on representations of the texts. In this lab, we'll introduce some different representations and you'll use nearest neighbor classification to predict the speaker of sentences selected from a children's book.\n","    \n","The objectives of this lab are to:\n","\n","* Clarify terminology around words and texts,\n","* Manipulate different representations of words and texts,\n","* Apply these representations to calculate text similarity, and\n","* Classify documents by a simple nearest neighbor model.\n","   \n","In this and later labs, we will have you carry out several exercises in notebook cells. The cells you are to do are marked '`#TODO`'. They will typically have a `...` where your code or answer should go. Where specified, you needn't write code to calculate the answer, but instead, simply work out the answer yourself and enter it."]},{"cell_type":"markdown","id":"c118a10c","metadata":{"id":"c118a10c"},"source":["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n","\n","* `math.acos`\n","* `math.pi`\n","* `re.match`\n","* `set`\n","* `sorted`\n","* `str.join`\n","* `str.lower`\n","* `torch.dot`\n","* `torch.linalg.norm`\n","* `torch.maximum`\n","* `torch.minimum`\n","* `torch.stack`\n","* `torch.sum`\n","* `torch.Tensor.type`\n","* `torch.where`\n","* `torch.zeros`\n","* `torch.zeros_like`\n","* `nltk.tokenize.word_tokenizer`\n","* `nltk.tokenize.WhitespaceTokenize`"]},{"cell_type":"markdown","id":"1284bb12","metadata":{"id":"1284bb12"},"source":["# Counting words\n","\n","<img src=\"https://github.com/nlp-course/data/blob/master/Seuss/seuss%20-%201966%20-%20green%20eggs%20and%20ham.gif?raw=true\" width=150 align=right />\n","\n","Here are five sentences from Dr. Seuss's [_Green Eggs and Ham_](https://en.wikipedia.org/wiki/Green_Eggs_and_Ham):\n","\n","    Would you like them here or there?\n","    I would not like them here or there.\n","    I would not like them anywhere.\n","    I do not like green eggs and ham.\n","    I do not like them, Sam-I-am.\n","\n","We'll make this text available in the variable `text`."]},{"cell_type":"code","execution_count":8,"id":"17e256f4","metadata":{"id":"17e256f4","executionInfo":{"status":"ok","timestamp":1681734712773,"user_tz":-180,"elapsed":268,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["text = \"\"\"\n","    Would you like them here or there?\n","    I would not like them here or there.\n","    I would not like them anywhere.\n","    I do not like green eggs and ham.\n","    I do not like them, Sam-I-am.\n","    \"\"\""]},{"cell_type":"markdown","id":"ef8f670e","metadata":{"deletable":false,"editable":false,"id":"ef8f670e"},"source":["A Python string like this is, of course, a sequence of characters. But we think of this text as a sequence of sentences each composed of a sequence of words. How many words are there in this text? That is a fraught question, for several reasons, including\n","\n","* The type-token distinction\n","* Tokenization issues\n","* Normalization\n","\n","## Types versus tokens\n","\n","In determining the number of words in `text`, are we talking about word _types_ or word _tokens_. (For instance, there are five _tokens_ of the word _type_ 'like'.)\n","\n","How many word tokens are there in total in this text? (Just count them manually.) Assign the number to the variable `token_count` in the next cell.\n","<!--\n","BEGIN QUESTION\n","name: token_count\n","-->"]},{"cell_type":"code","execution_count":22,"id":"b420ccfc","metadata":{"id":"b420ccfc","executionInfo":{"status":"ok","timestamp":1681735135284,"user_tz":-180,"elapsed":371,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO - define `token_count` to be the number of tokens in `text`\n","token_count = 15"]},{"cell_type":"code","execution_count":23,"id":"9ffb6cc5","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"9ffb6cc5","executionInfo":{"status":"ok","timestamp":1681735136926,"user_tz":-180,"elapsed":10,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"23666349-823f-449e-fbc0-7857ae853389"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":23}],"source":["grader.check(\"token_count\")"]},{"cell_type":"markdown","id":"a1299a50","metadata":{"deletable":false,"editable":false,"id":"a1299a50"},"source":["How many word types are there? (Again, you can just count manually.)\n","<!--\n","BEGIN QUESTION\n","name: type_count\n","-->"]},{"cell_type":"code","execution_count":29,"id":"18027f78","metadata":{"id":"18027f78","executionInfo":{"status":"ok","timestamp":1681735191685,"user_tz":-180,"elapsed":323,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO - define `type_count` to be the number of types in `text`\n","type_count = 35\n"]},{"cell_type":"code","execution_count":30,"id":"8c1837a4","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"8c1837a4","executionInfo":{"status":"ok","timestamp":1681735193311,"user_tz":-180,"elapsed":13,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"aa61ff68-5dfd-4ad2-a118-c3b8bc5ef01e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":30}],"source":["grader.check(\"type_count\")"]},{"cell_type":"markdown","id":"ef170119","metadata":{"deletable":false,"editable":false,"id":"ef170119"},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** The set of types of a language is referred to as its _vocabulary_. Are there more types or tokens as you calculated above? Could it be otherwise?\n","<!--\n","BEGIN QUESTION\n","name: type_vs_token_count\n","manual: true\n","-->"]},{"cell_type":"markdown","id":"718b9e4e","metadata":{"id":"718b9e4e"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","id":"cf0e9941","metadata":{"id":"cf0e9941"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","## Tokenization "]},{"cell_type":"markdown","id":"271da622","metadata":{"id":"271da622"},"source":["Did you count 'there?' as one token or two? This raises the issue of _tokenization_ of text, how to decide where the token boundaries occur. For instance, here's a simple way to split a string – to _tokenize_ it – in Python by splitting at whitespace."]},{"cell_type":"code","execution_count":27,"id":"2fb18584","metadata":{"id":"2fb18584","executionInfo":{"status":"ok","timestamp":1681735171355,"user_tz":-180,"elapsed":953,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["def whitespace_tokenize(str):\n","    return str.split()"]},{"cell_type":"markdown","id":"5ba2c5bc","metadata":{"deletable":false,"editable":false,"id":"5ba2c5bc"},"source":["Try it out on the `text` defined above.\n","<!--\n","BEGIN QUESTION\n","name: tokens_whitespace\n","-->"]},{"cell_type":"code","execution_count":31,"id":"cf11c9b3","metadata":{"id":"cf11c9b3","executionInfo":{"status":"ok","timestamp":1681735214620,"user_tz":-180,"elapsed":1011,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO - define `tokens` to be the tokens as defined by the `whitespace_tokenize` function\n","tokens = whitespace_tokenize(text)"]},{"cell_type":"code","execution_count":32,"id":"7ef2be11","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"7ef2be11","executionInfo":{"status":"ok","timestamp":1681735214621,"user_tz":-180,"elapsed":10,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"d1ba4141-d3fc-4d12-bdb3-d034848eedca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":32}],"source":["grader.check(\"tokens_whitespace\")"]},{"cell_type":"markdown","id":"9a566d23","metadata":{"deletable":false,"editable":false,"id":"9a566d23"},"source":["Using this tokenization method, count the number of tokens in the text, this time using Python to do the work.\n","<!--\n","BEGIN QUESTION\n","name: token_count_whitespace\n","-->"]},{"cell_type":"code","execution_count":33,"id":"493803ab","metadata":{"id":"493803ab","executionInfo":{"status":"ok","timestamp":1681735245716,"user_tz":-180,"elapsed":361,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO - place your token count here\n","token_count_2 = len(tokens)"]},{"cell_type":"code","execution_count":34,"id":"710a37ef","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"710a37ef","executionInfo":{"status":"ok","timestamp":1681735245717,"user_tz":-180,"elapsed":10,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"d92b89ec-93f2-423d-bba8-face207cd7ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":34}],"source":["grader.check(\"token_count_whitespace\")"]},{"cell_type":"markdown","id":"e3a59370","metadata":{"id":"e3a59370"},"source":["Arguably, we _should_ split off punctuation as separate tokens, but even there, some care must be taken. We don't want to split 'don't' into three tokens or 'Sam-I-Am' into five. (There's a good argument to be made however that the string 'don't' should be construed as two tokens, namely, 'do' and 'n't', but that's beyond the scope of today's discussion.)\n","\n","Here, we provide an alternative tokenizer that splits tokens at whitespace and splits off punctuation at the beginning and end of non-whitespace regions as separate tokens as well. It makes use of [the Python `re` module](https://docs.python.org/3/library/re.html) for regular expressions to specify the splitting process. Look over the code and make sure you understand what's going on. You might find [this online tool](https://regexr.com/) useful."]},{"cell_type":"code","execution_count":35,"id":"efbd32aa","metadata":{"id":"efbd32aa","executionInfo":{"status":"ok","timestamp":1681735257178,"user_tz":-180,"elapsed":310,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["def punc_tokenize(str):\n","    return [tok for tok in re.split('(\\W*?)\\s+', str) if tok != '']\n"]},{"cell_type":"markdown","id":"db167e71","metadata":{"deletable":false,"editable":false,"id":"db167e71"},"source":["Now how many tokens are there in the text if tokenized in this way?\n","<!--\n","BEGIN QUESTION\n","name: token_count_punc\n","-->"]},{"cell_type":"code","execution_count":36,"id":"4e0ca711","metadata":{"id":"4e0ca711","executionInfo":{"status":"ok","timestamp":1681735294034,"user_tz":-180,"elapsed":352,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","token_count_3 = len(punc_tokenize(text))"]},{"cell_type":"code","execution_count":37,"id":"9c74a353","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"9c74a353","executionInfo":{"status":"ok","timestamp":1681735294322,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"1709c9b4-2418-4fa1-a5fe-60a12bf32810"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":37}],"source":["grader.check(\"token_count_punc\")"]},{"cell_type":"markdown","id":"4345a81a","metadata":{"deletable":false,"editable":false,"id":"4345a81a"},"source":["## Normalization\n","\n","This tokenization method counts 'Would' and 'would' (capitalized and uncapitalized) as separate types. Is that a good idea? This raises the issue of text _normalization_.\n","\n","Define a function `normalize_token` that normalizes tokens by making them lowercase if at most the first character is uppercase. (Hints [here](https://docs.python.org/3/library/stdtypes.html#str.lower) and [here](https://docs.python.org/3/library/re.html#re.match). These are also listed in the hint cell at the top of the lab, so we'll mostly stop providing these hints from here on.)\n","<!--\n","BEGIN QUESTION\n","name: normalize_token\n","-->"]},{"cell_type":"code","execution_count":44,"id":"8389fc0e","metadata":{"id":"8389fc0e","executionInfo":{"status":"ok","timestamp":1681736494454,"user_tz":-180,"elapsed":3,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO - implement normalize_token, which returns the normalized word for a single word `str`\n","def normalize_token(str):\n","       token=str\n","       if len(token) > 1 and token[0].isupper() and not token[1:].isupper():\n","        return token.lower()\n","       else:\n","        return token"]},{"cell_type":"code","execution_count":45,"id":"b105d0b6","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"b105d0b6","executionInfo":{"status":"ok","timestamp":1681736496960,"user_tz":-180,"elapsed":296,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"e096228e-1581-49cd-a31c-245cb86d2a56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":45}],"source":["grader.check(\"normalize_token\")"]},{"cell_type":"markdown","id":"3fbf6b9e","metadata":{"deletable":false,"editable":false,"id":"3fbf6b9e"},"source":["Now define `norm_tokens_punc` to be the sequence of normalized tokens as tokenized by `punc_tokenize`\n","<!--\n","BEGIN QUESTION\n","name: norm_tokens_punc\n","-->"]},{"cell_type":"code","execution_count":46,"id":"3f3c87a4","metadata":{"id":"3f3c87a4","executionInfo":{"status":"ok","timestamp":1681736978398,"user_tz":-180,"elapsed":339,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","norm_tokens_punc = punc_tokenize(text)"]},{"cell_type":"code","execution_count":47,"id":"d12dcaf8","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"d12dcaf8","executionInfo":{"status":"ok","timestamp":1681736978705,"user_tz":-180,"elapsed":7,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"756e5a10-aa25-4e57-8995-e905d0482fff"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":47}],"source":["grader.check(\"norm_tokens_punc\")"]},{"cell_type":"markdown","id":"2b30684e","metadata":{"deletable":false,"editable":false,"id":"2b30684e"},"source":["How many types are there when tokenized and normalized in this way?\n","<!--\n","BEGIN QUESTION\n","name: type_count_norm_punc\n","-->"]},{"cell_type":"code","execution_count":48,"id":"23483fc1","metadata":{"id":"23483fc1","executionInfo":{"status":"ok","timestamp":1681736996412,"user_tz":-180,"elapsed":4,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","type_count_norm_punc = len(norm_tokens_punc)"]},{"cell_type":"code","execution_count":49,"id":"919fac6e","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"919fac6e","executionInfo":{"status":"ok","timestamp":1681736997654,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"a33f96b3-9c35-4fcf-ac13-050853bdf2be"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":49}],"source":["grader.check(\"type_count_norm_punc\")"]},{"cell_type":"markdown","id":"fa9efef6","metadata":{"id":"fa9efef6"},"source":["## Using prebuilt tokenizers\n","\n","Tokenization is so commonly needed that many packages provide pre-built tokenizers of various sorts.\n","We'll use one from [Natural Language Tool Kit (NLTK)](http://nltk.org). It's already been imported above under the name `nltk`."]},{"cell_type":"markdown","id":"c89fc85d","metadata":{"deletable":false,"editable":false,"id":"c89fc85d"},"source":["Define two tokenizers, versions of `whitespace_tokenize` and a normalized version of `punc_tokenize` above, using [the `nltk.tokenize.WhitespaceTokenizer`](https://www.nltk.org/api/nltk.tokenize.regexp.html#nltk.tokenize.regexp.WhitespaceTokenizer) and [`nltk.tokenize.word_tokenize`](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize) respectively.\n","Note that `nltk.tokenize.word_tokenize` only tokenizes the string, so we normalize the string by lowering the string's characters.\n","<!--\n","BEGIN QUESTION\n","name: nltk_whitespace_tokenize_and_nltk_normpunc_tokenize\n","-->"]},{"cell_type":"code","execution_count":76,"id":"95a21260","metadata":{"id":"95a21260","executionInfo":{"status":"ok","timestamp":1681738235025,"user_tz":-180,"elapsed":517,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","from nltk.tokenize import WhitespaceTokenizer\n","from nltk.tokenize import word_tokenize\n","def nltk_whitespace_tokenize(str):\n","    \n","    \n","    return WhitespaceTokenizer().tokenize(str)\n","     \n","    \n","def nltk_normpunc_tokenize(str):\n","    str = str.lower()\n","    return word_tokenize(str)\n","    "]},{"cell_type":"code","execution_count":77,"id":"ec3bd25a","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"ec3bd25a","executionInfo":{"status":"ok","timestamp":1681738236532,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"8b32f520-fcd2-4576-fdce-92b7efe62860"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":77}],"source":["grader.check(\"nltk_whitespace_tokenize_and_nltk_normpunc_tokenize\")"]},{"cell_type":"markdown","id":"1b0c0dd7","metadata":{"deletable":false,"editable":false,"id":"1b0c0dd7"},"source":["Now define `nltk_norm_tokens_punc` to be the sequence of normalized tokens as tokenized by the pre-built tokenizer `nltk_normpunc_tokenize`\n","<!--\n","BEGIN QUESTION\n","name: nltk_norm_tokens_punc\n","-->"]},{"cell_type":"code","execution_count":73,"id":"8de6f546","metadata":{"id":"8de6f546","executionInfo":{"status":"ok","timestamp":1681738168483,"user_tz":-180,"elapsed":818,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","nltk_norm_tokens_punc = nltk_normpunc_tokenize(text)"]},{"cell_type":"code","execution_count":74,"id":"23769f63","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"23769f63","executionInfo":{"status":"ok","timestamp":1681738168796,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"57b6ea2a-fb89-42d3-a7b9-2ae06f4132b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":74}],"source":["grader.check(\"nltk_norm_tokens_punc\")"]},{"cell_type":"markdown","id":"5c6ebacd","metadata":{"id":"5c6ebacd"},"source":["Now we should be able to print out the last few tokens of the sample text, tokenized using these functions."]},{"cell_type":"code","execution_count":78,"id":"44c50841","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44c50841","executionInfo":{"status":"ok","timestamp":1681738246330,"user_tz":-180,"elapsed":985,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"5ac2b012-f4e6-4aa0-d41c-0aa6b0c4bd46"},"outputs":[{"output_type":"stream","name":"stdout","text":["['green', 'eggs', 'and', 'ham.', 'I', 'do', 'not', 'like', 'them,', 'Sam-I-am.']\n","['ham', '.', 'i', 'do', 'not', 'like', 'them', ',', 'sam-i-am', '.']\n"]}],"source":["print(nltk_whitespace_tokenize(text)[-10:])\n","print(nltk_normpunc_tokenize(text)[-10:])"]},{"cell_type":"markdown","id":"4b40075a","metadata":{"id":"4b40075a"},"source":["> _Meta-comment:_ Because it's important that you get practice both with implementing the ideas in the course from first principles and also with using prebuilt software that provides similar functionality, we'll often have you engage in this seemingly redundant process of first implementing a small example and then applying a prebuilt method to do much the same thing. The effort may be duplicative, but it is not wasted."]},{"cell_type":"markdown","id":"ffd08746","metadata":{"id":"ffd08746"},"source":["# Representing words\n","\n","In this section, we'll explore some simple representations for tokens, as a step on the way to representing texts – sentences or documents:\n","\n","## String encoding\n","We've already seen string encoding above, representing a token of a word type by a string specific to that type: a token 'green' represented by an instance of the Python string `'green'`, for instance, or 'Sam-I-am' represented by `'Sam-I-am'`. So let's move on.\n","\n","## 1-hot encoding\n","Given a vocabulary for a language, we can associate each type with an integer, say by its index in a vector. We've already imported the `torch` module; we'll use `torch` tensors for the index vector. For the Seuss text, we can use this list to represent the ordered vocabulary:"]},{"cell_type":"code","execution_count":79,"id":"7f0b676a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7f0b676a","executionInfo":{"status":"ok","timestamp":1681738252560,"user_tz":-180,"elapsed":908,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"0e2a8b07-4cc4-404e-8973-639177346b0b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[',',\n"," '.',\n"," '?',\n"," 'and',\n"," 'anywhere',\n"," 'do',\n"," 'eggs',\n"," 'green',\n"," 'ham',\n"," 'here',\n"," 'i',\n"," 'like',\n"," 'not',\n"," 'or',\n"," 'sam-i-am',\n"," 'them',\n"," 'there',\n"," 'would',\n"," 'you']"]},"metadata":{},"execution_count":79}],"source":["vocabulary = sorted(set(nltk_norm_tokens_punc))\n","vocabulary"]},{"cell_type":"markdown","id":"535dbbdd","metadata":{"id":"535dbbdd"},"source":["### A digression on `torch` tensors\n","\n","Recall that `torch` tensors allow for vectorized computations: many operations on them work [componentwise](https://en.wikipedia.org/wiki/Pointwise#Componentwise_operations), that is, separately for each component of the tensor, rather than on the tensor all at once. Compare the following two operations, first on lists, then on `torch` tensors."]},{"cell_type":"code","execution_count":80,"id":"b04d1ad2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b04d1ad2","executionInfo":{"status":"ok","timestamp":1681738256111,"user_tz":-180,"elapsed":627,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"0e8f9d16-323c-476e-ec88-d034d007fc9e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":80}],"source":["[1, 2] == 1"]},{"cell_type":"code","execution_count":81,"id":"ea81a165","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ea81a165","executionInfo":{"status":"ok","timestamp":1681738257253,"user_tz":-180,"elapsed":398,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"0071cfae-c839-4a0a-da28-950035322c6b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ True, False])"]},"metadata":{},"execution_count":81}],"source":["torch.tensor([1, 2]) == 1"]},{"cell_type":"markdown","id":"e766e6b8","metadata":{"id":"e766e6b8"},"source":["This behavior of tensors is quite powerful, allowing for simply specifying complex operations and for efficient, even parallelizable, computation of them. You'll want ot take advantage of these characteristics of tensors where possible, here and in future assignments.\n","\n","But back to the 1-hot representation."]},{"cell_type":"markdown","id":"41a9d162","metadata":{"id":"41a9d162"},"source":["In the _1-hot representation_ of words, a token is then represented by a bit vector (again given as a `torch` tensor), with a 1 at the index of the token's type. (For consistency with some later `torch` functions, we'll take the elements to be floats rather than ints. The `.type` method is useful for converting the type, and it even conveniently broadcasts over the tensors.) For instance, the 1-hot representation of the comma token ',' would be"]},{"cell_type":"code","execution_count":82,"id":"360d6a82","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"360d6a82","executionInfo":{"status":"ok","timestamp":1681738259657,"user_tz":-180,"elapsed":401,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"4211942b-a155-4556-8c1e-ecb143a66f16"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0.])"]},"metadata":{},"execution_count":82}],"source":["torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).type(torch.float32)"]},{"cell_type":"markdown","id":"5314b43a","metadata":{"id":"5314b43a"},"source":["Conversion back and forth between these various representations is useful. Define functions `str_to_onehot` and `onehot_to_str` that convert between the string and one-hot representations using a vocabulary array to define the  conversion. \n","\n","Ideally, in your implementation, you'll want to take advantage of the componentwise nature of many tensor operations discussed above."]},{"cell_type":"code","execution_count":92,"id":"6c7dfd88","metadata":{"id":"6c7dfd88","executionInfo":{"status":"ok","timestamp":1681738720100,"user_tz":-180,"elapsed":320,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","def str_to_onehot(wordtype, vocab):\n","    \"\"\"Returns the 1-hot representation of `wordtype` with vocabulary \n","    `vocab`.\n","    The returned value should be a torch.tensor with data type float.\n","    \"\"\"\n","    \n","    onehot = torch.zeros(1, len(vocab))\n","    \n","    \n","    wordtype_index = vocab.index(wordtype)\n","    \n","   \n","    onehot[0, wordtype_index] = 1\n","    return onehot\n","\n","def onehot_to_str(onehot, vocab):\n","    \"\"\"Returns the string representation of `onehot`, a one-hot \n","    representation of a word type, with vocabulary `vocab`.\n","    \"\"\"\n","    index = onehot.argmax()\n","    # Use the index to retrieve the corresponding word from the vocabulary\n","    word = vocab[index]\n","    \n","    \n","    return word"]},{"cell_type":"markdown","id":"939cb540","metadata":{"deletable":false,"editable":false,"id":"939cb540"},"source":["Now use `str_to_onehot` to define the variable `anywhere_1hot` to be the 1-hot representation for a token of the type 'anywhere'. \n","<!--\n","BEGIN QUESTION\n","name: anywhere_1hot\n","-->"]},{"cell_type":"code","execution_count":93,"id":"434f67a1","metadata":{"id":"434f67a1","executionInfo":{"status":"ok","timestamp":1681738722154,"user_tz":-180,"elapsed":3,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","anywhere_1hot = str_to_onehot('anywhere',vocabulary)"]},{"cell_type":"code","execution_count":94,"id":"1cd13ffb","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"1cd13ffb","executionInfo":{"status":"ok","timestamp":1681738723908,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"0b47c346-46e5-48ff-e309-b7549e07616f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":94}],"source":["grader.check(\"anywhere_1hot\")"]},{"cell_type":"markdown","id":"011e3db1","metadata":{"deletable":false,"editable":false,"id":"011e3db1"},"source":["You can verify that the conversion worked correctly by inverting it using `onehot_to_str`, which we've done in the following unit test.\n","<!--\n","BEGIN QUESTION\n","name: anywhere_1hot_reverse\n","-->"]},{"cell_type":"code","execution_count":95,"id":"623fda8b","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"623fda8b","executionInfo":{"status":"ok","timestamp":1681738724372,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"ffaf92a0-2dea-4e64-b74f-e9a0e85f566b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":95}],"source":["anywhere_1hot_reverse=onehot_to_str(anywhere_1hot,vocabulary)\n","\n","grader.check(\"anywhere_1hot_reverse\")"]},{"cell_type":"markdown","id":"a72e9715","metadata":{"id":"a72e9715"},"source":["# Representing texts\n","\n","## The set-of-words representation\n","\n","We can represent a whole text (a sequence of words) by manipulating the vector representations of the words within the text. For instance, we can take the componentwise maximum of the vectors. We refer to this as the _set-of-words_ representation.\n","\n","Here we've defined a function `set_of_words` that returns the set of words representation for a token sequence."]},{"cell_type":"code","execution_count":96,"id":"c5b4d849","metadata":{"id":"c5b4d849","executionInfo":{"status":"ok","timestamp":1681739002832,"user_tz":-180,"elapsed":306,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["def set_of_words(tokens, vocabulary):\n","    \"\"\"Returns the set-of-words representation as a tensor of floats for the \n","    sequence of `tokens` using the `vocabulary` to specify the conversion.\n","    \"\"\"\n","    onehots = torch.stack([str_to_onehot(token, vocabulary) for token in tokens])\n","    return torch.amax(onehots, 0).type(torch.float32)"]},{"cell_type":"markdown","id":"e55abd17","metadata":{"id":"e55abd17"},"source":["This representation for a text is a vector that has a `1` for each word type that occurs in the text. The vector represents the [characteristic function](https://en.wikipedia.org/wiki/Characteristic_function) for the subset of vocabulary words that appear in the text; hence the term 'set of words'.\n"]},{"cell_type":"markdown","id":"a58a8fa4","metadata":{"deletable":false,"editable":false,"id":"a58a8fa4"},"source":["What is the set-of-words representation for the example text 'I would not, would not, here or there.'?\n","<!--\n","BEGIN QUESTION\n","name: example_sow\n","-->"]},{"cell_type":"code","execution_count":103,"id":"aae3d445","metadata":{"id":"aae3d445","executionInfo":{"status":"ok","timestamp":1681739172559,"user_tz":-180,"elapsed":4,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO - define the variable to be the set of words representation for the example text\n","example_text= 'I would not, would not, here or there.'\n","# Use `nltk_normpunc_tokenize` tokenizer\n","example_sow = set_of_words(nltk_normpunc_tokenize(example_text),vocabulary)"]},{"cell_type":"code","execution_count":104,"id":"8a12a44c","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"8a12a44c","executionInfo":{"status":"ok","timestamp":1681739175579,"user_tz":-180,"elapsed":465,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"7c577bc7-f950-41a6-c228-b1245a12563a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":104}],"source":["grader.check(\"example_sow\")"]},{"cell_type":"markdown","id":"16495c0a","metadata":{"id":"16495c0a"},"source":["## The bag of words representation\n","\n","If instead, we take the componentwise _addition_ of the vectors instead of the maximum, the text representation provides the _frequency_ of each word type in the text. We refer to this representation as the _[bag](https://en.wikipedia.org/wiki/Multiset) of words_ representation. \n","\n","Define a function `bag_of_words`, analogous to `set_of_words` above, that returns the bag-of-words representation for a token sequence."]},{"cell_type":"code","execution_count":105,"id":"ddb042a9","metadata":{"id":"ddb042a9","executionInfo":{"status":"ok","timestamp":1681739207829,"user_tz":-180,"elapsed":1140,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","def bag_of_words(tokens, vocabulary):\n","    onehots = torch.stack([str_to_onehot(token, vocabulary) for token in tokens])\n","    return torch.sum(onehots, 0).type(torch.float32)"]},{"cell_type":"markdown","id":"6ce9a236","metadata":{"deletable":false,"editable":false,"id":"6ce9a236"},"source":["What is the bag of words representation for the example text 'I would not, would not, here or there.'?\n","<!--\n","BEGIN QUESTION\n","name: example_bow\n","-->"]},{"cell_type":"code","execution_count":106,"id":"97c69a91","metadata":{"id":"97c69a91","executionInfo":{"status":"ok","timestamp":1681739226979,"user_tz":-180,"elapsed":304,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO - define the variable to be the bag of words representation for the example text\n","# 'I would not, would not, here or there.'\n","# Use the `nltk_normpunc_tokenize` tokenizer\n","example_bow = bag_of_words(nltk_normpunc_tokenize(example_text),vocabulary)"]},{"cell_type":"code","execution_count":107,"id":"a28bee58","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"a28bee58","executionInfo":{"status":"ok","timestamp":1681739229839,"user_tz":-180,"elapsed":416,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"9ba51935-fd9e-4c43-9529-58b0d5d4c7bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":107}],"source":["grader.check(\"example_bow\")"]},{"cell_type":"markdown","id":"9e297489","metadata":{"id":"9e297489"},"source":["# Document similarity metrics\n","\n","Consider the following text classification problem: Each sentence in _Green Eggs and Ham_ is spoken by one of two characters, Sam-I-Am and Guy-Am-I. We want to be able to classify new sentences as (most likely) being uttered by one of the two.\n","\n","A simple method for text classification is the _nearest neighbor_ method. We select the class for the new sentence that is the same as the class of the \"nearest\" (most similar) sentence for which we already know the class. (You'll experiment much more with this text classification method in the next lab.)\n","\n","To perform nearest neighbor classification, we need a method for measuring the (metaphorical) distance between two texts based on their representations. We'll explore a few methods here:\n","\n","* Hamming distance\n","* Jaccard distance\n","* Euclidean distance\n","* cosine distance\n","\n","You'll implement code for all of these distance metrics. Try to implement the functions using `torch` tensor functions only, without explicit iteration over the elements in the vector.\n","\n","We'll take a look at the distances among the following sentences:\n","\n","1. Would you like them here or there?\n","2. I would not like them here or there.\n","3. Do you like green eggs and ham?\n","4. I do not like them Sam-I-Am.\n","\n","We'll start with the set of words representations of these sentences:"]},{"cell_type":"code","execution_count":108,"id":"ea7ef901","metadata":{"id":"ea7ef901","executionInfo":{"status":"ok","timestamp":1681739235371,"user_tz":-180,"elapsed":302,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["examples = \"\"\"Would you like them here or there?\n","              I would not like them here or there.\n","              Do you like green eggs and ham?\n","              I do not like them Sam-I-Am.\"\"\" \\\n","           .split(\"\\n\")\n","sows = [set_of_words(nltk_normpunc_tokenize(sentence), vocabulary) \n","            for sentence in examples]"]},{"cell_type":"code","execution_count":109,"id":"1836d353","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1836d353","executionInfo":{"status":"ok","timestamp":1681739235862,"user_tz":-180,"elapsed":6,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"5a5e0b02-de4e-4755-bf39-c27ba901402e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n","          1.]]),\n"," tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n","          0.]]),\n"," tensor([[0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n","          1.]]),\n"," tensor([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n","          0.]])]"]},"metadata":{},"execution_count":109}],"source":["sows"]},{"cell_type":"markdown","id":"62950a96","metadata":{"deletable":false,"editable":false,"id":"62950a96"},"source":["## Hamming distance\n","\n","The Hamming distance between two vectors is the number of positions at which they differ. Define a function `hamming_distance` that computes the Hamming distance between two vectors.\n","<!--\n","BEGIN QUESTION\n","name: hamming_distance\n","-->"]},{"cell_type":"code","execution_count":113,"id":"9b03e409","metadata":{"id":"9b03e409","executionInfo":{"status":"ok","timestamp":1681739404373,"user_tz":-180,"elapsed":1265,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO - implement hamming_distance. The returned value should be an integer.\n","def hamming_distance(v1, v2):\n","    if v1.shape != v2.shape:\n","        raise ValueError(\"Tensors must have the same shape.\")\n","    distance = torch.sum(v1 != v2).float()\n","    return distance"]},{"cell_type":"code","execution_count":114,"id":"e5209a16","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"e5209a16","executionInfo":{"status":"ok","timestamp":1681739404830,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"31e9baa9-b3dc-4733-eb40-df0a8ccf97bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":114}],"source":["grader.check(\"hamming_distance\")"]},{"cell_type":"markdown","id":"b237323b","metadata":{"id":"b237323b"},"source":["Now we can generate the Hamming distances among all of the sample sentences in a little table. Do the values make sense?"]},{"cell_type":"code","execution_count":115,"id":"06b70da4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06b70da4","executionInfo":{"status":"ok","timestamp":1681739411314,"user_tz":-180,"elapsed":312,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"18883d24-0169-4249-a2b3-7e5e079aec5e"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 0.0  5.0 10.0 11.0 \n"," 5.0  0.0 15.0  6.0 \n","10.0 15.0  0.0 11.0 \n","11.0  6.0 11.0  0.0 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{hamming_distance(sows[i], sows[j]):4} \", end='')\n","    print()"]},{"cell_type":"markdown","id":"a59e2847","metadata":{"deletable":false,"editable":false,"id":"a59e2847"},"source":["## Jaccard distance\n","\n","The Jaccard distance between two sets (and remember that these bit strings basically represent sets) is one minus the number of elements in their intersection divided by the number of elements in their union.\n","\n","$$ D_{jaccard}(v_1, v_2) = 1 - \\frac{| v_1 \\cap v_2 |}{| v_1 \\cup v_2 |} $$\n","\n","Define a function `jaccard_distance` to compute the Jaccard distance between two set-of-words representations.\n","<!--\n","BEGIN QUESTION\n","name: jaccard_distance\n","-->"]},{"cell_type":"code","execution_count":172,"id":"fe97a03d","metadata":{"id":"fe97a03d","executionInfo":{"status":"ok","timestamp":1681742147589,"user_tz":-180,"elapsed":1269,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","def jaccard_distance(v1, v2):\n","   \n","    \n","    s1 = set(torch.nonzero(v1).flatten().tolist())\n","    s2 = set(torch.nonzero(v2).flatten().tolist())\n","    # compute the size of the intersection and union of the two sets\n","    intersection_size = len(s1.intersection(s2))\n","    union_size = len(s1.union(s2))\n","    # compute the Jaccard distance between the two sets\n","    jaccard_distance = 1 - intersection_size / union_size\n","    return torch.tensor(jaccard_distance)"]},{"cell_type":"code","execution_count":173,"id":"2795d7e2","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"2795d7e2","executionInfo":{"status":"ok","timestamp":1681742148001,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"101f17bf-a4b9-45c0-a4e5-2a5c8ee92710"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":173}],"source":["grader.check(\"jaccard_distance\")"]},{"cell_type":"markdown","id":"0348e6e7","metadata":{"id":"0348e6e7"},"source":["Again, here's a table of the Jaccard distances among the sample sentences."]},{"cell_type":"code","execution_count":174,"id":"0d7ff497","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d7ff497","executionInfo":{"status":"ok","timestamp":1681742153852,"user_tz":-180,"elapsed":312,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"5bf964ee-c8f1-469b-abba-d804bc5b55d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 0.417 0.714 0.786 \n","0.417 0.000 0.882 0.500 \n","0.714 0.882 0.000 0.786 \n","0.786 0.500 0.786 0.000 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{jaccard_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"]},{"cell_type":"markdown","id":"86854a8c","metadata":{"deletable":false,"editable":false,"id":"86854a8c"},"source":["## Euclidean distance\n","\n","The Euclidean distance between two vectors is the norm of the vector between them, that is,\n","\n","$$ D_{euclidean}(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} - \\mathbf{y}| $$\n","\n","where $|\\mathbf{z}|$, the norm of a vector $\\mathbf{z}$, is calculated as\n","\n","$$ |\\mathbf{z}| = \\sqrt{\\sum_{i=1}^N \\mathbf{z}_i^2} $$\n","\n","Fortunately, `torch` provides the function [`torch.linalg.norm`](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm) to compute the norm, and the vector between two vectors can be computed by componentwise subtraction.\n","\n","Define a function `euclidean_distance` to compute the Euclidean distance between two vectors.\n","<!--\n","BEGIN QUESTION\n","name: euclidean_distance\n","-->"]},{"cell_type":"code","execution_count":151,"id":"bc7ea99e","metadata":{"id":"bc7ea99e","executionInfo":{"status":"ok","timestamp":1681741644346,"user_tz":-180,"elapsed":428,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","def euclidean_distance(v1, v2):\n","    diff = v1-v2\n","    return torch.linalg.norm(diff)"]},{"cell_type":"code","execution_count":152,"id":"3dea5b02","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"3dea5b02","executionInfo":{"status":"ok","timestamp":1681741646227,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"e85adeda-31d0-4248-9446-d50b0f2ed075"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":152}],"source":["grader.check(\"euclidean_distance\")"]},{"cell_type":"markdown","id":"c7e6d704","metadata":{"id":"c7e6d704"},"source":["Again, here's a table of the Euclidean distances among the sample sentences."]},{"cell_type":"code","execution_count":153,"id":"7ac20734","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ac20734","executionInfo":{"status":"ok","timestamp":1681741648646,"user_tz":-180,"elapsed":291,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"2374ba0b-90fd-4601-d194-94aca9c2cf9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 2.236 3.162 3.317 \n","2.236 0.000 3.873 2.449 \n","3.162 3.873 0.000 3.317 \n","3.317 2.449 3.317 0.000 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{euclidean_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"]},{"cell_type":"markdown","id":"f8c5ace6","metadata":{"id":"f8c5ace6"},"source":["## Cosine distance\n","\n","The _cosine similarity_ of two vectors of length $N$ is the cosine of the angle that they form, which is computed as the dot product of the two vectors divided by their norms.\n","\n","$$ cos(\\mathbf{x}, \\mathbf{y}) = \n","      \\frac{\\sum_{i=1}^N \\mathbf{x}_i \\cdot \\mathbf{y}_i}{|\\mathbf{x}| \\cdot |\\mathbf{y}|} $$\n","\n","This isn't a distance metric, but a similarity metric. For vectors of non-negative numbers, it ranges from 0 to 1, where 0 is maximally different and 1 is maximally similar. To turn it into a distance metric, then, we take the inverse cosine (to convert the cosine to an angle between $\\pi$ and 0) and divide by $\\pi$.\n","\n","$$ D_{cosine}(\\mathbf{x}, \\mathbf{y}) = \\frac{cos^{-1}(cos(\\mathbf{x}, \\mathbf{y}))}{\\pi} $$\n","\n","Since we're using `torch`, some of these functions are already provided. See hints [here](https://pytorch.org/docs/stable/generated/torch.dot.html), [here](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html), and [here](https://docs.python.org/3/library/math.html#math.acos).\n","\n","(To avoid some math domain errors, we recommend that you use the function `safe_acos` that we've provided to compute the inverse cosine function instead of using `math.acos` directly.)"]},{"cell_type":"code","execution_count":154,"id":"9618dc72","metadata":{"id":"9618dc72","executionInfo":{"status":"ok","timestamp":1681741657533,"user_tz":-180,"elapsed":394,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["def safe_acos(x):\n","    \"\"\"Returns the arc cosine of `x`. Unlike `math.acos`, it \n","       does not raise an exception for values of `x` out of range, \n","       but rather clips `x` at -1..1, thereby avoiding math domain\n","       errors in the case of numerical errors.\"\"\"\n","    return math.acos(math.copysign(min(1.0, abs(x)), x))"]},{"cell_type":"code","execution_count":167,"id":"22ded3f7","metadata":{"id":"22ded3f7","executionInfo":{"status":"ok","timestamp":1681741972325,"user_tz":-180,"elapsed":613,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"outputs":[],"source":["#TODO\n","def cosine_distance(v1, v2):\n","     \n","    v1 = torch.tensor(v1).view(-1)\n","    v2 = torch.tensor(v2).view(-1)\n","    # compute the dot product of the two vectors\n","    dot_product = torch.dot(v1, v2)\n","    # compute the norms of the two vectors\n","    norm_v1 = torch.norm(v1)\n","    norm_v2 = torch.norm(v2)\n","    # compute the cosine similarity between the two vectors\n","    cosine_similarity = dot_product / (norm_v1 * norm_v2)\n","    # convert the cosine similarity to cosine distance\n","    cosine_distance = safe_acos(cosine_similarity) / torch.pi\n","    return cosine_distance"]},{"cell_type":"code","execution_count":168,"id":"b75201fd","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"b75201fd","executionInfo":{"status":"ok","timestamp":1681741973262,"user_tz":-180,"elapsed":10,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"3cdf0a56-c25b-4e46-9cf2-fabce970ba69"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":168}],"source":["grader.check(\"cosine_distance\")"]},{"cell_type":"code","execution_count":169,"id":"d88a3333","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d88a3333","executionInfo":{"status":"ok","timestamp":1681741974962,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"e825b3bb-dc61-4724-94b7-50bb20b75860"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 0.250 0.378 0.414 \n","0.250 0.000 0.462 0.283 \n","0.378 0.462 0.000 0.414 \n","0.414 0.283 0.414 0.000 \n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-167-bda09e248d86>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  v1 = torch.tensor(v1).view(-1)\n","<ipython-input-167-bda09e248d86>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  v2 = torch.tensor(v2).view(-1)\n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{cosine_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"]},{"cell_type":"markdown","id":"c6cf44d8","metadata":{"id":"c6cf44d8"},"source":["In the next lab, you'll use some of these distance metrics to automatically classify text using nearest neighbor classification."]},{"cell_type":"markdown","id":"a14894e2","metadata":{"deletable":false,"editable":false,"id":"a14894e2"},"source":["<!-- BEGIN QUESTION -->\n","\n","# Lab debrief\n","\n","**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n","\n","* Was the lab too long or too short?\n","* Were the readings appropriate for the lab? \n","* Was it clear (at least after you completed the lab) what the points of the exercises were? \n","* Are there additions or changes you think would make the lab better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"]},{"cell_type":"markdown","id":"7a51c5ff","metadata":{"id":"7a51c5ff"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","id":"eea4fe3b","metadata":{"id":"eea4fe3b"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","# Submission Instructions\n","\n","This lab should be submitted to Gradescope. Submit both the code [here](https://www.gradescope.com/courses/522028?submit_assignment_id=2745476) and the pdf [here](https://www.gradescope.com/courses/522028?submit_assignment_id=1927894), or by logging in to the course page on Gradescope. \n","\n","Make sure that you have passed all public tests by running `grader.check_all()` below before submitting. Note that there are hidden tests on Gradescope, the results of which will be revealed after the submission deadline."]},{"cell_type":"markdown","id":"751f7be4","metadata":{"id":"751f7be4"},"source":["# End of lab 1-1"]},{"cell_type":"markdown","id":"78a8f5d7","metadata":{"deletable":false,"editable":false,"id":"78a8f5d7"},"source":["---\n","\n","To double-check your work, the cell below will rerun all of the autograder tests."]},{"cell_type":"code","execution_count":175,"id":"1ea49682","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":864},"id":"1ea49682","executionInfo":{"status":"ok","timestamp":1681742164345,"user_tz":-180,"elapsed":463,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"59af85e5-5d81-4797-cc31-ab8871a9e621"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["anywhere_1hot:\n","\n","    All tests passed!\n","    \n","\n","anywhere_1hot_reverse:\n","\n","    All tests passed!\n","    \n","\n","cosine_distance:\n","\n","    All tests passed!\n","    \n","\n","euclidean_distance:\n","\n","    All tests passed!\n","    \n","\n","example_bow:\n","\n","    All tests passed!\n","    \n","\n","example_sow:\n","\n","    All tests passed!\n","    \n","\n","hamming_distance:\n","\n","    All tests passed!\n","    \n","\n","jaccard_distance:\n","\n","    All tests passed!\n","    \n","\n","nltk_norm_tokens_punc:\n","\n","    All tests passed!\n","    \n","\n","nltk_whitespace_tokenize_and_nltk_normpunc_tokenize:\n","\n","    All tests passed!\n","    \n","\n","norm_tokens_punc:\n","\n","    All tests passed!\n","    \n","\n","normalize_token:\n","\n","    All tests passed!\n","    \n","\n","token_count:\n","\n","    All tests passed!\n","    \n","\n","token_count_punc:\n","\n","    All tests passed!\n","    \n","\n","token_count_whitespace:\n","\n","    All tests passed!\n","    \n","\n","tokens_whitespace:\n","\n","    All tests passed!\n","    \n","\n","type_count:\n","\n","    All tests passed!\n","    \n","\n","type_count_norm_punc:\n","\n","    All tests passed!\n","    \n"],"text/html":["<p><strong>anywhere_1hot:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>anywhere_1hot_reverse:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>cosine_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>euclidean_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>example_bow:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>example_sow:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>hamming_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>jaccard_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>nltk_norm_tokens_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>nltk_whitespace_tokenize_and_nltk_normpunc_tokenize:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>norm_tokens_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>normalize_token:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count_whitespace:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>tokens_whitespace:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>type_count:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>type_count_norm_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n"]},"metadata":{},"execution_count":175}],"source":["grader.check_all()"]}],"metadata":{"celltoolbar":"Tags","kernelspec":{"display_name":"otter-latest","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"title":"CS236299 Lab 1-1: Types, tokens, and representing text","vscode":{"interpreter":{"hash":"4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}