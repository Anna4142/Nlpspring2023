{"cells":[{"cell_type":"code","source":["!gdown --id 1DQldYmyunCKSgtJDoxJDE1E4vYBFG0Mp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkglIS9Gr2G4","executionInfo":{"status":"ok","timestamp":1683428985168,"user_tz":-180,"elapsed":1632,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"63aeabce-12bb-40b9-d3bf-1417e1a9d1c3"},"id":"bkglIS9Gr2G4","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1DQldYmyunCKSgtJDoxJDE1E4vYBFG0Mp\n","To: /content/requirements.txt\n","100% 83.0/83.0 [00:00<00:00, 411kB/s]\n"]}]},{"cell_type":"code","source":["!pip install -r /content/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jj1P7loDsB5i","executionInfo":{"status":"ok","timestamp":1683429011754,"user_tz":-180,"elapsed":26616,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"97bba41b-4d7a-49ed-ee5b-00d7ef5c0b9b"},"id":"jj1P7loDsB5i","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 1)) (2.0.0+cu118)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 2)) (3.7.1)\n","Collecting otter-grader==1.0.0\n","  Downloading otter_grader-1.0.0-py3-none-any.whl (163 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.0/164.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 4)) (1.5.3)\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (7.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (67.7.2)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (5.8.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (3.1.2)\n","Collecting dill\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.5.4)\n","Collecting docker\n","  Downloading docker-6.1.0-py3-none-any.whl (147 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.65.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.0)\n","Collecting pdfkit\n","  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/requirements.txt (line 4)) (1.22.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/requirements.txt (line 4)) (2022.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 1)) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 1)) (3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 1)) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/requirements.txt (line 1)) (2.0.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r /content/requirements.txt (line 1)) (16.0.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r /content/requirements.txt (line 1)) (3.25.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 2)) (8.4.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 2)) (4.39.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 2)) (0.11.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 2)) (1.0.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 2)) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 2)) (23.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 2)) (1.4.4)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/requirements.txt (line 5)) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/requirements.txt (line 5)) (2022.10.31)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/requirements.txt (line 6)) (2023.4.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/requirements.txt (line 6)) (9.0.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/requirements.txt (line 6)) (2.0.12)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/requirements.txt (line 6)) (23.1.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r /content/requirements.txt (line 4)) (1.16.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/requirements.txt (line 5)) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/requirements.txt (line 5)) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/requirements.txt (line 5)) (3.4)\n","Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.5.1)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (3.0.38)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (5.7.1)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.1.6)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.14.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.1.2)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.2.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.2.2)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.7.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.0.0)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (5.3.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.9.2)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.7.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (1.5.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.11.2)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.4)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (4.3.3)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.16.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r /content/requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.8.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.19.3)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (3.3.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (6.1.12)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.2.6)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (2.4.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (0.5.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r /content/requirements.txt (line 3)) (23.2.1)\n","Installing collected packages: tokenizers, pdfkit, xxhash, PyPDF2, multidict, jedi, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, docker, aiosignal, transformers, aiohttp, otter-grader, datasets\n","Successfully installed PyPDF2-3.0.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 docker-6.1.0 frozenlist-1.3.3 huggingface-hub-0.14.1 jedi-0.18.2 multidict-6.0.4 multiprocess-0.70.14 otter-grader-1.0.0 pdfkit-1.0.0 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["!pip install wget"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFrBSuIQsaQT","executionInfo":{"status":"ok","timestamp":1683429019711,"user_tz":-180,"elapsed":7968,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"7ee78c12-28f7-472b-d9fd-feb9f16b1d80"},"id":"UFrBSuIQsaQT","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=5e45d6bbe0bd99a1dfe6711afbd989ba0d91f2324e143fa31e81f3112d91e3e0\n","  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}]},{"cell_type":"code","execution_count":null,"id":"94d3b18e","metadata":{"collapsed":true,"deletable":false,"editable":false,"jupyter":{"outputs_hidden":true,"source_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"94d3b18e","executionInfo":{"status":"ok","timestamp":1683429022730,"user_tz":-180,"elapsed":3032,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"e368647d-947f-4139-c182-6b9f1bae1e81"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}],"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2023-spring/project1.git .tmp\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"]},{"cell_type":"code","execution_count":null,"id":"d5027944","metadata":{"deletable":false,"editable":false,"id":"d5027944"},"outputs":[],"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"]},{"cell_type":"raw","id":"2bfe698f","metadata":{"jupyter":{"source_hidden":true},"id":"2bfe698f"},"source":["%%latex\n","\\newcommand{\\vect}[1]{\\mathbf{#1}}\n","\\newcommand{\\cnt}[1]{\\sharp(#1)}\n","\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\newcommand{\\softmax}{\\operatorname{softmax}}\n","\\newcommand{\\Prob}{\\Pr}\n","\\newcommand{\\given}{\\,|\\,}"]},{"cell_type":"markdown","id":"2563869b","metadata":{"id":"2563869b"},"source":["$$\n","\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n","\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n","\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\renewcommand{\\softmax}{\\operatorname{softmax}}\n","\\renewcommand{\\Prob}{\\Pr}\n","\\renewcommand{\\given}{\\,|\\,}\n","$$"]},{"cell_type":"markdown","id":"8f82eb20","metadata":{"id":"8f82eb20","tags":["remove_for_latex"]},"source":["# Course 236299\n","\n","## Project segment 1: Text classification\n","\n","In this project segment you will build several varieties of text classifiers using PyTorch.\n","\n","1. A majority baseline.\n","2. A naive Bayes classifer.\n","3. A logistic regression (single-layer perceptron) classifier.\n","4. A multilayer perceptron classifier."]},{"cell_type":"markdown","id":"82ba2a8f","metadata":{"id":"82ba2a8f"},"source":["# Preparation"]},{"cell_type":"code","execution_count":null,"id":"f575f075","metadata":{"deletable":false,"editable":false,"id":"f575f075"},"outputs":[],"source":["import copy\n","import re\n","import wget\n","import csv\n","import torch\n","import torch.nn as nn\n","import datasets\n","\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers import normalizers\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from transformers import PreTrainedTokenizerFast\n","from collections import Counter\n","from torch import optim\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"id":"d1a92710","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1a92710","executionInfo":{"status":"ok","timestamp":1683429029194,"user_tz":-180,"elapsed":12,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"a3e3af90-d04b-4043-9745-981564910fd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["# Random seed\n","random_seed = 1234\n","torch.manual_seed(random_seed)\n","\n","## GPU check\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","id":"b0e162e4","metadata":{"id":"b0e162e4"},"source":["# The task: Answer types for ATIS queries\n","\n","For this and future project segments, you will be working with a standard natural-language-processing dataset, the [ATIS (Airline Travel Information System) dataset](https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk). This dataset is composed of queries about flights – their dates, times, locations, airlines, and the like.\n","\n","Over the years, the dataset has been annotated in all kinds of ways, with parts of speech, informational chunks, parse trees, and even corresponding SQL database queries. You'll use various of these annotations in future assignments. For this project segment, however, you'll pursue an easier classification task: **given a query, predict the answer type**.\n","\n","These queries ask for different types of answers, such as\n","\n","* Flight IDs: \"Show me the flights from Washington to Boston\"\n","* Fares: \"How much is the cheapest flight to Milwaukee\"\n","* City names: \"Where does flight 100 fly to?\"\n","\n","In all, there are some 30 answer types to the queries.\n","\n","Below is an example taken from this dataset:\n","\n","_Query:_\n","\n","```\n","show me the afternoon flights from washington to boston\n","```\n","\n","_SQL:_\n","\n","```\n","SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport_service airport_service_1 , city city_1 , airport_service airport_service_2 , city city_2 \n","   WHERE flight_1.departure_time BETWEEN 1200 AND 1800 \n","     AND ( flight_1.from_airport = airport_service_1.airport_code \n","           AND airport_service_1.city_code = city_1.city_code \n","           AND city_1.city_name = 'WASHINGTON' \n","           AND flight_1.to_airport = airport_service_2.airport_code \n","           AND airport_service_2.city_code = city_2.city_code \n","           AND city_2.city_name = 'BOSTON' )\n","```\n","\n","In this project segment, we will consider the answer type for a natural-language query to be the target field of the corresponding SQL query. For the above example, the answer type would be *flight_id*."]},{"cell_type":"markdown","id":"193b2abd","metadata":{"id":"193b2abd"},"source":["## Loading and preprocessing the data\n","\n","> Read over this section, executing the cells, and **making sure you understand what's going on before proceeding to the next parts.**\n","\n","First, let's download the dataset."]},{"cell_type":"code","execution_count":null,"id":"3f47d516","metadata":{"id":"3f47d516"},"outputs":[],"source":["data_dir = \"https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/\"\n","os.makedirs('data', exist_ok=True)\n","for split in ['train', 'dev', 'test']:\n","    wget.download(f\"{data_dir}/{split}.nl\", out='data/')\n","    wget.download(f\"{data_dir}/{split}.sql\", out='data/')"]},{"cell_type":"markdown","id":"6b50b7d3","metadata":{"id":"6b50b7d3"},"source":["Next, we process the dataset by extracting answer types from SQL queries and saving in CSV format."]},{"cell_type":"code","execution_count":null,"id":"064ca5f1","metadata":{"id":"064ca5f1"},"outputs":[],"source":["def get_label_from_query(query):\n","    \"\"\"Returns the answer type from `query` by dead reckoning.\n","    It's basically the second or third token in the SQL query.\n","    \"\"\"    \n","    match = re.match(r'\\s*SELECT\\s+(DISTINCT\\s*)?(\\w+\\.)?(?P<label>\\w+)', query)\n","    if match:\n","        label = match.group('label')\n","    else:\n","        raise RuntimeError(f'no label in query {query}')\n","    return label\n","\n","for split in ['train', 'dev', 'test']:\n","    sql_file = f'data/{split}.sql'\n","    nl_file = f'data/{split}.nl'\n","    out_file = f'data/{split}.csv'\n","    \n","    with open(nl_file) as f_nl:\n","        with open(sql_file) as f_sql:\n","            with open(out_file, 'w') as fout:\n","                writer = csv.writer(fout)\n","                writer.writerow(('label','text'))\n","                for text, sql in zip(f_nl, f_sql):\n","                    text = text.strip()\n","                    sql = sql.strip()\n","                    label = get_label_from_query(sql)\n","                    writer.writerow((label, text))"]},{"cell_type":"markdown","id":"4db1eb37","metadata":{"id":"4db1eb37"},"source":["Let's take a look at what the data file looks like."]},{"cell_type":"code","execution_count":null,"id":"6d49c307","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6d49c307","executionInfo":{"status":"ok","timestamp":1683429041171,"user_tz":-180,"elapsed":4,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"35d2488e-e42f-43b1-d20b-888ecc9394eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["label,text\n","flight_id,list all the flights that arrive at general mitchell international from various cities\n","flight_id,give me the flights leaving denver august ninth coming back to boston\n","flight_id,what flights from tacoma to orlando on saturday\n","fare_id,what is the most expensive one way fare from boston to atlanta on american airlines\n","flight_id,what flights return from denver to philadelphia on a saturday\n","flight_id,can you list all flights from chicago to milwaukee\n","flight_id,show me the flights from denver that go to pittsburgh and then atlanta\n","flight_id,i'd like to see flights from baltimore to atlanta that arrive before noon and i'd like to see flights from denver to atlanta that arrive before noon\n","flight_id,do you have an 819 flight from denver to san francisco\n"]}],"source":["shell('head \"data/train.csv\"')"]},{"cell_type":"markdown","id":"d20e26e8","metadata":{"id":"d20e26e8"},"source":["We use `datasets` to prepare the data, as in lab 1-5. More information on `datasets` can be found at [https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)."]},{"cell_type":"code","execution_count":null,"id":"a19aa89e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169,"referenced_widgets":["6b31c1d9ba4d4b599dd0a258e3f443a1","442ed2e84c534a529ae79a98334a7cb3","0c862ced84bb4fc5ac665b08a69154c7","a512dfeda574445cad455b796a0e6e80","253922cc8a09457694f7a7d98317650b","8f77de170a10484e9f8dab918d15c17e","79c610c0cf9b4375ac4fe8528c9a69bc","8ea0d180721f489eb10daa77a8ddf78c","2f6ebc1e599b41fe943bc11988ce87f1","e92c5f91d41845f7adc5a9331c707530","a181c889744d4a20863f3c40f7c17647","c614e8add25e4c87a41250217b930d0f","3837a9f2be294fb690eb9dd4cbcdbc9e","743add99ef7e4092b45d848792dada4a","e77723ad8c1843e6aa6c8c12cf3d27f4","5ffe96baa832488d8377f8f283b5b56c","8e4ec5ffac3a4362b8f4fd68301526c2","b1af7b86c42148a685e2b810aea4013a","41599c8908d246d3b59e91cb4767572c","41c7618482f047ef8effda0ea2ae41b6","5e497cc46ab9455ba59e4b58d97bf187","22a2122d70194653ab053878b7004a93","e4ebea9c16584fa7ac0194dd32314669","d3a6858ff7f046a08ce496c1ef4594d6","28ab661c633c4f9bb54aac1923f9e6a3","252ded762b9d4071a978fc11f55c0bf7","d0108b78398941098e4f32cf362cc842","940c2fd105394735917455fb08ed3eff","d16398f1672b42d182582a919a646033","2f47bec29de343348e3a7b8d85110544","740f90f1cf95466a822b6e1746149c30","44d1fd1128034ae09e2f3af5d3e8def4","8c56f7e6482e45ac9f4fb3e028ca5066","b195f2b9906a489c935bcf2c50949dd7","a5158791422b4b92802a30aa11e6781c","c8cd2be8f9b04b87854189609bb6e8e1","a096eaa18ccb41b89ee8ef0f3dc6a465","8c7e3d632594412fa7eb0dbcdc8c3c6c","9c9510f4b92a4e0bac46e6ac1b70b06c","bfc24ac767ef4c5c8ab72b2b5131d67f","6d4bbff2d2e34a1ab79880f7b78d1dd6","b1b84104b302437d92ee5e841e85d44f","4fa10e0214724d789719cfd0b55fbb39","acf06df9ffae4eb992fbb62aa38d9d72","7de46e69049a4438a7cc20285097dbae","b155d9ee8e034f4a96fd27ed966c45a5","ba0db8f6975b4f02a7a281d6bc1304d4","f8115349e88b4ad39f04fc04111dc2f7","5765c609aaf9455095efe1e3a9c57da3","83c2ce70cf1f41358d87d36b5927c062","cf4bfb9c99c44ffc94aeddae15699e1c","4a1e8baf68964500a47faca8d9129346","ece0ec209ac146cf8449ab8c128b6d24","05b7bb6048374287979886a4217738dc","ca503967579340ba90e2ec59df2b9952","cdfb94c92945459c852bed7f0b6e48ba","d76d0ce0f14f40fba45044585530d538","962b6affc164451580ae95efc8dc41c4","588deb16d373427a8f2930296b2a80b4","ef87c7678ee340659082590fd7e2d575","33d8f24e6068414781b76fbe749987f7","6f862421bd1c404ba3ea3247afe25a03","4a7b645163584d0ba11aa85479ca5158","08093e96fbb94fb08083b939ae0a9075","90bb6be83a9d404e8601f5c423f431f5","d911dd7309344cfc8c33af93797cf30f"]},"id":"a19aa89e","executionInfo":{"status":"ok","timestamp":1683429052885,"user_tz":-180,"elapsed":1787,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"318d1a81-9e0e-4e79-bac7-db01ac21c170"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-251ad8b40911d51d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b31c1d9ba4d4b599dd0a258e3f443a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c614e8add25e4c87a41250217b930d0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ebea9c16584fa7ac0194dd32314669"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating val split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b195f2b9906a489c935bcf2c50949dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7de46e69049a4438a7cc20285097dbae"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-251ad8b40911d51d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdfb94c92945459c852bed7f0b6e48ba"}},"metadata":{}}],"source":["atis = load_dataset('csv', data_files={'train':'data/train.csv', \\\n","                                       'val': 'data/dev.csv', \\\n","                                       'test': 'data/test.csv'})"]},{"cell_type":"code","execution_count":null,"id":"f8206b76","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8206b76","executionInfo":{"status":"ok","timestamp":1683429055973,"user_tz":-180,"elapsed":5,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"72b16581-dd7d-426c-c0c9-e9cecde27895"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['label', 'text'],\n","        num_rows: 4379\n","    })\n","    val: Dataset({\n","        features: ['label', 'text'],\n","        num_rows: 491\n","    })\n","    test: Dataset({\n","        features: ['label', 'text'],\n","        num_rows: 448\n","    })\n","})"]},"metadata":{},"execution_count":14}],"source":["atis"]},{"cell_type":"code","execution_count":null,"id":"04d4310a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04d4310a","executionInfo":{"status":"ok","timestamp":1683429060648,"user_tz":-180,"elapsed":484,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"b4a03e72-1980-4838-9ef0-5aa04b894a9e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['label', 'text'],\n","    num_rows: 4379\n","})"]},"metadata":{},"execution_count":15}],"source":["train_data = atis['train']\n","val_data = atis['val']\n","test_data = atis['test']\n","\n","train_data.shuffle(seed=random_seed)"]},{"cell_type":"markdown","id":"a03dc6d5","metadata":{"id":"a03dc6d5"},"source":["We build a tokenizer from the training data to tokenize text and convert tokens into word ids."]},{"cell_type":"code","execution_count":null,"id":"cee1cbb3","metadata":{"id":"cee1cbb3"},"outputs":[],"source":["MIN_FREQ = 3 # words appearing fewer than 3 times are treated as 'unknown'\n","unk_token = '[UNK]'\n","pad_token = '[PAD]'\n","\n","tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n","tokenizer.pre_tokenizer = Whitespace()\n","tokenizer.normalizer = normalizers.Lowercase()\n","\n","trainer = WordLevelTrainer(min_frequency=MIN_FREQ, special_tokens=[pad_token, unk_token])\n","tokenizer.train_from_iterator(train_data['text'], trainer=trainer)"]},{"cell_type":"markdown","id":"eb44f8c1","metadata":{"id":"eb44f8c1"},"source":["We use `datasets.Dataset.map` to convert text into word ids. As shown in lab 1-5, first we need to wrap `tokenizer` with the `transformers.PreTrainedTokenizerFast` class to be compatible with the `datasets` library."]},{"cell_type":"code","execution_count":null,"id":"f24daa66","metadata":{"id":"f24daa66"},"outputs":[],"source":["hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, pad_token=pad_token, unk_token=unk_token)"]},{"cell_type":"code","execution_count":null,"id":"7162f8ce","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["238a73d0a964436c97d26b523ac0fbbd","ce926881a4914c0baae64d7104c9a6b6","560ada30dd604f1596e7a37bdd32f42a","6a8a258a1f2a4562b7a5e6cb1f560c47","1b5fb14e522645848f46774a73a49c63","06ebd8677bf9432fa3456100b8ed109f","711fe84328d7475fb63f7612c874ac48","4f252a2ae2594ed28003d2732af4fa7f","7d63eb0a0df4432589a0db8a8f0c253a","ca58c5af24e842be8796152ca5392f63","612b8a71634946c69b434087b16e9068","0f9e92bd5c304383871caf826a57997e","b252ad85fe814531a35d37779c4a093f","d29a50fd3eee4f039cb43620108f6639","8da4e9f56dfc46248892be262b9e1dbc","4dd74845659242fab2255df6eccada2d","43f6be5e3bc142628405b0389fbcf1d4","0a04dd213d624da3bb616a7e25439622","3bb5dffd6e4b42ff89301ff1594e47dc","93fa46f749184ef3812eacd6a362bb85","0e6f5ea6f9634b6aac26c2a5a5d313dd","4d616e684f8944e3919f1af38b5a0b22","982fc37a0ef94033b7241d9d6dd53fcc","96de8c4c1f474d55bf530f4932646f6c","94df51c894df42ba8739394114de8399","fdde3081c3464bb4994fbe13676dbc1e","1534ef26d5a1455b9ada1f72b14191f5","bcd1f3a835334bcb8d97645b365e7c45","17c7afd026564bab91af26d282dd8586","403e5d980b2a41fd88c7b72e8d9ce570","0bbf1414a2d04656a9ac4fa7d9283905","b1d982d35297489885c680347ed27c00","23f0d9eb62e249fba72e81f1711c94e4"]},"id":"7162f8ce","executionInfo":{"status":"ok","timestamp":1683429069533,"user_tz":-180,"elapsed":15,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"9b840cc6-8198-49f0-a70a-a7975bd5b744"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4379 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"238a73d0a964436c97d26b523ac0fbbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/491 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9e92bd5c304383871caf826a57997e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/448 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"982fc37a0ef94033b7241d9d6dd53fcc"}},"metadata":{}}],"source":["def encode(example):\n","    return hf_tokenizer(example['text'])\n","\n","train_data = train_data.map(encode)\n","val_data = val_data.map(encode)\n","test_data = test_data.map(encode)"]},{"cell_type":"code","source":["print(train_data[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvOIIuSZ_kL9","executionInfo":{"status":"ok","timestamp":1683429070837,"user_tz":-180,"elapsed":5,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"c478105d-efa0-445b-864c-e9941a80d2ac"},"id":"IvOIIuSZ_kL9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'label': 'flight_id', 'text': 'list all the flights that arrive at general mitchell international from various cities', 'input_ids': [29, 24, 5, 4, 32, 80, 71, 213, 214, 176, 3, 1, 335], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}]},{"cell_type":"markdown","id":"641434d6","metadata":{"id":"641434d6"},"source":["We also need to convert label strings into label ids."]},{"cell_type":"code","execution_count":null,"id":"2eb10575","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["8188158a16b94fb895dfc24760b0d77f","a816269269f54715b57dca731c2b491a","5861ab1504344814943bc733d0777492","30942ddf3b1247eda624557f9b6900ba","2b8ec7cf73ac4955b7b982b2c11f506b","ea904870136b4a939110c33fa217e6d6","653b82686f9540e9a96d45f090f8b341","6f7ab527cc3642ba889d7d0bf7c2ea65","19a018421e3e42ea9e2bcaa29fb16c68","822084aa16e947f9815b3933c29925ad","a897114130a847be92ea5ed7e2ba604d","93e566573e4247458126803d58ec5541","4e4eaf5eb59b47b2b38717806e1e13d1","ebcd884284d04defb93715fb4993d19b","eb83da846708445baee136f8fecfc55a","d8785f811d284561a65f7a2915da284e","694e364f094747b98255504015f90a82","4811f369c2e746e98d856aee9c41b193","85e293b33a0f47a4bfab2f381f50d12d","381fb5a103bc4120b1185383e8b65b70","3e4460c5f9ee424c818ad72cb3d241b2","dd5f387430fe46ce983854e5fe0969de","10b61ed346f84e1cac668c4460aeed05","9f0cf899c7b44fdaae425030781cf102","b78122fda9a74554b493579e9710ff29","74518cc52d474ddb926e3b8f918bb68c","74b6e6551ab043a891ce53c53ae16aa6","61da12044fe846d2a45bc87a1d664909","60cd8f08b7204bd5aeb0eb157dc63c10","0f5ca0f7c9af4ac89e65416ac2f7650a","c3a7314b798e42b99645f55066bd81d6","a098411a668340c884acc5d048b95dd3","a2fb9dbc8eaa4edc8479400450e721cb","c764ff7168c74ab2b52b90dd026073c7","bc558a80958b4510bb07451b0262b623","974dd5f5627e4312be489db18df40429","a34dbed3c01741c1836217d608456134","aab7b83a08c542a8b9c2c6a7361c6fd1","c7a6a9e6881541bc963072bdd286310b","41375e74f98b42ecb1bf856be1351b52","accb31441c2e4946b13baf02498a3b20","246e1debf56a4f2baed2cab5bc741fd6","362d044fd30b47a5bc9b1e29d312cbcb","720e34f67cf04964b6aa232f56799511","7dae38cf09f44dd1b80f748c8d95f1f4","3c6085707ab9430082c73156345b33c7","de19507e13c44eb497379b59be0c2456","63743c0621c640f9a68e87ba4a5bff73","d83fdde704e34e9ba46e431e51ae8059","d7e01a5f424345b392be336ab53a2ea3","d2e798cefcea4f26ab993822916a3ae6","a91e3061831c40e495214eb19be5fdff","4fa2ccb5a31b4ae892628f2c64b10a8b","bcc8ed5d26024d0bb152e74577716be2","019b62ae29de4bcf870e2d282517077f"]},"id":"2eb10575","executionInfo":{"status":"ok","timestamp":1683429074472,"user_tz":-180,"elapsed":1597,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"b77913bc-1309-4d6a-eb3c-a1713de59ad6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Casting to class labels:   0%|          | 0/4379 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8188158a16b94fb895dfc24760b0d77f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Casting to class labels:   0%|          | 0/491 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93e566573e4247458126803d58ec5541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Aligning the labels:   0%|          | 0/491 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b61ed346f84e1cac668c4460aeed05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Casting to class labels:   0%|          | 0/448 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c764ff7168c74ab2b52b90dd026073c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Aligning the labels:   0%|          | 0/448 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dae38cf09f44dd1b80f748c8d95f1f4"}},"metadata":{}}],"source":["# Add a new column `label_id`\n","train_data = train_data.add_column('label_id', train_data['label'])\n","val_data = val_data.add_column('label_id', val_data['label'])\n","test_data = test_data.add_column('label_id', test_data['label'])\n","\n","# Convert feature `label_id` from strings to integer ids\n","train_data = train_data.class_encode_column('label_id')\n","\n","# Use the label vocabulary on training data to convert val and test sets\n","label2id = train_data.features['label_id']._str2int\n","val_data = val_data.class_encode_column('label_id')\n","val_data = val_data.align_labels_with_mapping(label2id, \"label_id\")\n","test_data = test_data.class_encode_column('label_id')\n","test_data = test_data.align_labels_with_mapping(label2id, \"label_id\")"]},{"cell_type":"code","execution_count":null,"id":"09f9c110","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09f9c110","executionInfo":{"status":"ok","timestamp":1683429076243,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"ead0c5fc-d7d5-426d-bb03-631228d7be15"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'cincinnati': 186, 'do': 106, 'late': 246, '6': 262, 'houston': 126, 'business': 253, 'ua': 301, 's': 101, 'the': 5, 'sfo': 422, 'by': 152, 'restriction': 249, 'there': 45, 'logan': 377, 'out': 233, 'vegas': 102, '8pm': 252, '9': 347, 'daily': 174, 'ap80': 474, 'american': 68, 'numbers': 416, 'all': 24, 'abbreviation': 390, 'lowest': 204, 'seventeenth': 357, 'that': 32, 'monday': 73, 'york': 76, '[PAD]': 0, 'plane': 274, 'transportation': 51, 'help': 448, 'tuesdays': 460, 'close': 477, 'columbus': 203, 'as': 230, 'o': 107, 'oakland': 43, 'evening': 91, 'milwaukee': 96, 'stand': 293, 'right': 342, 'jfk': 449, 'minnesota': 491, 'class': 53, 'way': 67, 'stop': 108, 'weekday': 513, 'trip': 55, 'february': 395, 'going': 142, 'newark': 93, 'fifteenth': 280, 'where': 317, 'shortest': 345, 'pittsburgh': 20, 'layover': 353, 'q': 399, 'march': 354, 'mornings': 413, '2': 387, 'we': 512, 'eighth': 264, 'meal': 192, 'ap57': 433, 'into': 191, '1100': 429, 'am': 221, 'hp': 375, 'cost': 141, 'tuesday': 84, 'international': 176, 'latest': 132, '8am': 287, 'dallas': 22, '57': 319, 'over': 378, 'jersey': 325, '1991': 251, 'number': 248, '11am': 431, 'leave': 70, 'next': 121, 'include': 486, 'classes': 321, 'takeoffs': 401, '3': 363, 'these': 458, 'thursday': 79, 'display': 289, 'will': 267, 'second': 205, 'has': 244, 'april': 238, 'meals': 328, 'airport': 86, 'prices': 281, 'night': 209, 'mco': 340, 'could': 170, 'boeing': 436, '296': 465, 'wednesdays': 302, 'ewr': 372, '2100': 464, 'serve': 201, 'ground': 48, 'delta': 97, 'same': 343, 'let': 396, 'northwest': 200, 'provided': 418, 'how': 88, 'stopover': 144, 'pm': 211, '1992': 462, 'does': 52, '5pm': 148, 'early': 124, 'arrange': 391, 'dl': 268, 'schedule': 235, 'types': 308, 'but': 410, 'long': 166, 'services': 500, 'live': 412, 'nonstop': 78, 'ticket': 159, 'each': 394, '10am': 182, 'chicago': 109, 'leaves': 165, 'through': 295, 'north': 313, 'fit': 482, 'turboprop': 511, 'and': 19, 'be': 227, 'coming': 437, 'thirtieth': 294, '466': 365, 'new': 69, 'twentieth': 283, 'limousine': 326, 'burbank': 239, 'fourth': 255, 'nineteenth': 356, 'francisco': 17, 'hi': 338, 'downtown': 135, 'coach': 195, 'louis': 177, 'lunch': 327, 'need': 54, 'go': 82, 'morning': 37, 'price': 257, 'midnight': 452, 'thrift': 426, 'is': 21, 'd': 46, 'code': 131, 'stopping': 258, 'of': 31, 'flights': 4, 'get': 153, 'now': 218, 'airfare': 270, 'what': 7, '4': 286, 'other': 306, '7pm': 202, '825': 470, 'far': 444, '12': 183, 'trying': 402, '[UNK]': 1, 'know': 171, 'salt': 139, 'most': 273, 'tell': 129, 'list': 29, 'jose': 168, 'planes': 379, 'destination': 440, 'field': 224, 'use': 403, '7am': 469, 'afternoon': 63, 'december': 228, 'kinds': 487, 'people': 398, 'offer': 453, 'm': 154, 'washington': 34, 'which': 64, 'saturdays': 498, 'sundays': 505, 'fourteenth': 373, 'via': 276, 'then': 360, 'you': 50, 'round': 58, 'fares': 66, 'toronto': 111, 'makes': 339, '1115am': 430, 'seats': 499, 'fare': 39, 'stops': 236, '8': 279, 'goes': 256, 'wednesday': 47, 'see': 145, 'january': 351, 'traveling': 427, 'connect': 478, 'arrive': 80, 'please': 36, 'airports': 242, 'sixth': 292, 'united': 81, 'saturday': 119, 'clock': 110, 'type': 167, 'departures': 439, 'baltimore': 23, 'express': 310, 'y': 362, 'twa': 160, 'show': 11, 'possible': 234, 'denver': 14, 'following': 446, 'los': 138, 'cheapest': 49, 'beach': 185, '1000': 278, 'information': 83, 'boston': 10, 'angeles': 136, 'two': 332, 'at': 71, 'hello': 485, 'some': 358, 'would': 38, 'uses': 404, 'when': 383, 'll': 450, 'county': 297, 'seating': 344, 'either': 349, '5': 220, 'any': 99, '1245pm': 432, 'detroit': 196, 'one': 65, 'indianapolis': 127, 'eastern': 263, 'airlines': 30, 'ap': 296, 'i': 12, 'describe': 481, 'booking': 409, 'arriving': 57, 'used': 206, 'cleveland': 140, 'florida': 323, 'friday': 117, 'take': 250, 'midwest': 312, '747': 406, 'are': 28, 'cheap': 475, 'air': 123, 'have': 60, '630am': 467, 'flying': 212, 'july': 113, 'rental': 282, 'dinner': 254, 'only': 305, 'carolina': 309, '281': 388, 'tickets': 316, '4pm': 261, '.': 89, 'bwi': 304, 'flight': 9, 'okay': 179, 'thank': 359, 'me': 8, '10pm': 333, 'san': 13, 'much': 178, 'flies': 298, 'diego': 112, 'mitchell': 214, 'trips': 510, 'area': 434, 'tampa': 163, 'petersburg': 151, 'near': 493, '838': 471, 'yes': 428, 'costs': 479, '7': 366, 'rent': 421, 'dc': 75, 'an': 156, 'arrivals': 435, 'looking': 247, 'breakfast': 223, 'october': 341, 'travel': 173, '6pm': 184, 'also': 271, 'again': 303, 'passengers': 454, 'montreal': 146, 'taxi': 506, '3pm': 364, 'dollars': 175, 'another': 473, 'my': 355, 'nw': 495, 'between': 33, 'city': 56, 'options': 496, 'or': 169, 'no': 397, 'ff': 445, 'dfw': 348, 'it': 225, 'landings': 352, 'eighteenth': 443, 'h': 324, 'nashville': 172, 'today': 300, 'tomorrow': 133, 'great': 484, 'interested': 245, 'qo': 419, 'phoenix': 125, 'serving': 400, 'on': 6, 'explain': 350, 'us': 122, 'listing': 299, 'third': 219, 'la': 241, 'eleventh': 322, 'served': 329, 'transport': 361, 'car': 240, 'may': 147, 'anywhere': 407, 'miami': 95, 'three': 508, 'smallest': 315, 'yn': 405, 'before': 61, '1': 384, 'connecting': 243, 'times': 237, \"'\": 27, 'philadelphia': 25, 'arrives': 222, 'twelfth': 331, 'departing': 164, 'ontario': 266, 'service': 193, 'worth': 120, 'with': 62, 'twenty': 92, 'economy': 197, 'after': 44, 'fifth': 265, 'sunday': 98, 'capacity': 334, 'their': 425, 'returning': 291, 'thursdays': 509, 'westchester': 285, 'tacoma': 180, 'day': 207, 'direct': 231, 'name': 414, 'st': 90, 'airplane': 367, 'ea': 442, 'depart': 190, 'minneapolis': 162, 'later': 411, 'qw': 420, 'georgia': 447, 'memphis': 155, 'orlando': 114, 'can': 85, 'under': 346, '934pm': 472, 'sometime': 502, 'tennessee': 507, 'november': 199, 'rates': 456, 'arrangements': 272, '9am': 288, 'should': 330, 'dc10': 480, 'stands': 504, 'if': 376, '718am': 468, 'available': 59, '10': 385, 'during': 441, 'kind': 217, 'from': 3, 'sixteenth': 381, 'book': 188, 'fort': 118, 'sorry': 503, 'making': 451, 'around': 143, 'ninth': 290, 'using': 275, 'for': 41, 'charlotte': 103, 'departure': 370, 'land': 488, 'tenth': 307, 'total': 459, 'qx': 314, 'cities': 335, 'southwest': 423, 'paul': 215, 'hours': 374, 'love': 226, 'lufthansa': 489, '9pm': 389, 'want': 74, 'seventh': 115, 'canada': 393, '1291': 461, 'more': 492, 'noon': 94, 'nationair': 415, 'about': 150, 'august': 100, 'seattle': 128, 'june': 149, 'this': 269, 'codes': 369, 'colorado': 336, 're': 380, 'fn': 483, 'make': 232, 'reservation': 497, '12pm': 318, 'your': 277, 'arrival': 408, 'stopovers': 424, 'lake': 137, 'september': 210, 'both': 392, 'like': 26, 'leaving': 35, 'less': 208, 'choices': 476, 'airline': 116, 'atlanta': 18, 'back': 320, 'california': 194, 'kansas': 130, 'general': 213, '430pm': 466, 'continental': 105, 'requesting': 457, 'least': 189, '2pm': 260, 'find': 87, 'week': 284, 'noontime': 494, 'f': 311, 'six': 501, 'las': 104, 'many': 157, 'mean': 158, 'distance': 371, 'last': 198, 'in': 16, 'serves': 216, 'fly': 40, 'earliest': 77, 'aircraft': 134, 'pennsylvania': 455, '/': 259, 'first': 42, 'ohio': 417, 'expensive': 161, 'thirty': 382, 'canadian': 368, 'meaning': 490, '1pm': 386, 'time': 187, '21': 463, 'guardia': 337, 'give': 72, 'than': 181, 'a': 15, 'departs': 438, 'to': 2, 'return': 229}\n","{'advance_purchase': 0, 'aircraft_code': 1, 'airline_code': 2, 'airport_code': 3, 'airport_location': 4, 'arrival_time': 5, 'basic_type': 6, 'booking_class': 7, 'city_code': 8, 'city_name': 9, 'count': 10, 'day_name': 11, 'departure_time': 12, 'fare_basis_code': 13, 'fare_id': 14, 'flight_id': 15, 'flight_number': 16, 'ground_fare': 17, 'meal_code': 18, 'meal_description': 19, 'miles_distant': 20, 'minimum_connect_time': 21, 'minutes_distant': 22, 'restriction_code': 23, 'state_code': 24, 'stop_airport': 25, 'stops': 26, 'time_elapsed': 27, 'time_zone_code': 28, 'transport_type': 29}\n","Size of vocab: 514\n","Number of labels: 30\n"]}],"source":["# Compute size of vocabulary\n","text_vocab = tokenizer.get_vocab()\n","label_vocab = train_data.features['label_id']._str2int\n","vocab_size = len(text_vocab)\n","num_labels = len(label_vocab)\n","print(text_vocab)\n","print(label_vocab)\n","print(f\"Size of vocab: {vocab_size}\")\n","print(f\"Number of labels: {num_labels}\")"]},{"cell_type":"markdown","id":"8cc57196","metadata":{"id":"8cc57196"},"source":["To get a sense of the kinds of things that are asked about in this dataset, here is the list of all of the answer types in the training data."]},{"cell_type":"code","execution_count":null,"id":"5dcc16c1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dcc16c1","executionInfo":{"status":"ok","timestamp":1683310653494,"user_tz":-180,"elapsed":34,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"8b1a3a7f-0729-4368-dc56-f472759e9253"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 0 advance_purchase\n"," 1 aircraft_code\n"," 2 airline_code\n"," 3 airport_code\n"," 4 airport_location\n"," 5 arrival_time\n"," 6 basic_type\n"," 7 booking_class\n"," 8 city_code\n"," 9 city_name\n","10 count\n","11 day_name\n","12 departure_time\n","13 fare_basis_code\n","14 fare_id\n","15 flight_id\n","16 flight_number\n","17 ground_fare\n","18 meal_code\n","19 meal_description\n","20 miles_distant\n","21 minimum_connect_time\n","22 minutes_distant\n","23 restriction_code\n","24 state_code\n","25 stop_airport\n","26 stops\n","27 time_elapsed\n","28 time_zone_code\n","29 transport_type\n"]}],"source":["for label in label_vocab:\n","    print(f\"{label_vocab[label]:2d} {label}\") "]},{"cell_type":"markdown","id":"2c724fd7","metadata":{"id":"2c724fd7"},"source":["## Handling unknown words\n","\n","Note that we mapped words appearing fewer than 3 times to a special _unknown_ token (we're using `[UNK]`) for two reasons: \n","\n","1. Due to the scarcity of such rare words in training data, we might not be able to learn generalizable conclusions about them.\n","2. Introducing an unknown token allows us to deal with out-of-vocabulary words in the test data as well: we just map those words to `[UNK]`."]},{"cell_type":"code","execution_count":null,"id":"bfbc6ccb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfbc6ccb","executionInfo":{"status":"ok","timestamp":1683429080952,"user_tz":-180,"elapsed":5,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"010b2e68-e348-494b-d393-22aba549ae1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unknown token: [UNK]\n","Unknown token id: 1\n","An unknown token: IAmAnUnknownWordForSure\n","Mapped back to word id: [1]\n","Mapped to [UNK]'s?: True\n"]}],"source":["print (f\"Unknown token: {unk_token}\")\n","unk_index = text_vocab[unk_token]\n","print (f\"Unknown token id: {unk_index}\")\n","\n","# UNK example\n","example_unk_token = 'IAmAnUnknownWordForSure'\n","print (f\"An unknown token: {example_unk_token}\")\n","print (f\"Mapped back to word id: {hf_tokenizer(example_unk_token).input_ids}\")\n","print (f\"Mapped to [UNK]'s?: {all([id == unk_index for id in hf_tokenizer(example_unk_token).input_ids])}\")"]},{"cell_type":"markdown","id":"094f121f","metadata":{"id":"094f121f"},"source":["To facilitate batching sentences of different lengths into the same tensor as we'll see later, we also reserved a special padding symbol `[PAD]`."]},{"cell_type":"code","execution_count":null,"id":"0731b1da","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0731b1da","executionInfo":{"status":"ok","timestamp":1683429089727,"user_tz":-180,"elapsed":834,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"0bd03e5c-0391-45e4-dbe5-f66247d23d15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Padding token: [PAD]\n","Padding token id: 0\n"]}],"source":["print (f\"Padding token: {pad_token}\")\n","pad_index = text_vocab[pad_token]\n","print (f\"Padding token id: {pad_index}\")"]},{"cell_type":"markdown","id":"21e601dc","metadata":{"id":"21e601dc"},"source":["## Batching the data\n","\n","To load data in batches, we use `torch.utils.data.DataLoader`. This enables us to iterate over the dataset under a given `BATCH_SIZE` which specifies how many examples we want to process at a time."]},{"cell_type":"code","execution_count":null,"id":"58a49bc8","metadata":{"id":"58a49bc8"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","# Defines how to batch a list of examples together\n","def collate_fn(examples):\n","    batch = {}\n","    bsz = len(examples)\n","    label_ids = []\n","    for example in examples:\n","        label_ids.append(example['label_id'])\n","    label_batch = torch.LongTensor(label_ids).to(device)\n","    input_ids = []\n","    for example in examples:\n","        input_ids.append(example['input_ids'])\n","    max_length = max([len(word_ids) for word_ids in input_ids])\n","    text_batch = torch.zeros(bsz, max_length).long().fill_(pad_index).to(device)\n","    for b in range(bsz):\n","        text_batch[b][:len(input_ids[b])] = torch.LongTensor(input_ids[b]).to(device)\n","    \n","    batch['label_ids'] = label_batch\n","    batch['input_ids'] = text_batch\n","    return batch\n","\n","train_iter = torch.utils.data.DataLoader(train_data, \n","                                         batch_size=BATCH_SIZE,\n","                                         collate_fn=collate_fn)\n","val_iter = torch.utils.data.DataLoader(val_data, \n","                                       batch_size=BATCH_SIZE, \n","                                       collate_fn=collate_fn)\n","test_iter = torch.utils.data.DataLoader(test_data, \n","                                        batch_size=BATCH_SIZE, \n","                                        collate_fn=collate_fn)"]},{"cell_type":"markdown","id":"f2667709","metadata":{"id":"f2667709"},"source":["Let's look at a single batch from one of these iterators."]},{"cell_type":"code","execution_count":null,"id":"9de1f906","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9de1f906","executionInfo":{"status":"ok","timestamp":1683429108486,"user_tz":-180,"elapsed":10083,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"97ba45e0-f7b9-45c2-f168-38a4d2e8f31e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of text batch: torch.Size([32, 31])\n","Third sentence in batch: tensor([  7,   4,   3, 180,   2, 114,   6, 119,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0], device='cuda:0')\n","Mapped back to string: what flights from tacoma to orlando on saturday [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","Mapped back to string skipping padding: what flights from tacoma to orlando on saturday\n","Size of label batch: torch.Size([32])\n","Third label in batch: 15\n","Mapped back to string: flight_id\n"]}],"source":["batch = next(iter(train_iter))\n","text = batch['input_ids']\n","print (f\"Size of text batch: {text.size()}\")\n","print (f\"Third sentence in batch: {text[2]}\")\n","print (f\"Mapped back to string: {hf_tokenizer.decode(text[2])}\")\n","print (f\"Mapped back to string skipping padding: {hf_tokenizer.decode(text[2], skip_special_tokens=True)}\")\n","\n","label = batch['label_ids']\n","label_vocab_itos = train_data.features['label_id']._int2str # map from label ids to strs\n","print (f\"Size of label batch: {label.size()}\")\n","print (f\"Third label in batch: {label[2]}\")\n","print (f\"Mapped back to string: {label_vocab_itos[label[2].item()]}\")"]},{"cell_type":"markdown","id":"496e57c6","metadata":{"id":"496e57c6"},"source":["You might notice some padding tokens `[PAD]` when we convert word ids back to strings, or equivalently, padding ids `0` in the corresponding tensor. The reason why we need such padding is because the sentences in a batch might be of different lengths, and to save them in a 2D tensor for parallel processing, sentences that are shorter than the longest sentence need to be padded with some placeholder values. Later during training you'll need to make sure that the paddings do not affect the final results."]},{"cell_type":"markdown","id":"3ef52e05","metadata":{"id":"3ef52e05"},"source":["Alternatively, we can also directly iterate over the individual examples in `train_data`, `val_data` and `test_data`. Here the returned values are the raw sentences and labels instead of their corresponding ids, and you might need to explicitly deal with the unknown words, unlike using bucket iterators which automatically map unknown words to an unknown word id."]},{"cell_type":"code","execution_count":null,"id":"27ead4c9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"27ead4c9","executionInfo":{"status":"ok","timestamp":1683429108488,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"26d3fde6-db59-404d-825b-c0871ed46607"},"outputs":[{"output_type":"stream","name":"stdout","text":["flight_id  -- list all the flights that arrive at general mitchell international from various cities\n","flight_id  -- give me the flights leaving denver august ninth coming back to boston\n","flight_id  -- what flights from tacoma to orlando on saturday\n","fare_id    -- what is the most expensive one way fare from boston to atlanta on american airlines\n","flight_id  -- what flights return from denver to philadelphia on a saturday\n"]}],"source":["for _, example in zip(range(5), train_data):\n","  print(f\"{example['label']:10} -- {example['text']}\")"]},{"cell_type":"markdown","id":"1cdbafc8","metadata":{"id":"1cdbafc8"},"source":["## Notations used\n","\n","In this project segment, we'll use the following notations.\n","\n","* Sequences of elements (vectors and the like) are written with angle brackets and commas ($\\langle w_1, \\ldots, w_M \\rangle$) or directly with no punctuation ($w_1 \\cdots w_M$).\n","* Sets are notated similarly but with braces, ($\\{ v_1, \\ldots, v_V \\}$).\n","* Maximum indices ($M$, $N$, $V$, $T$, and $X$ in the following) are written as uppercase italics.\n","* Variables over sequences and sets are written in boldface ($\\vect{w}$), typically with the same letter as the variables over their elements.\n","\n","In particular,\n","\n","* $\\vect{w} = w_1 \\cdots w_M$: A text to be classified, each element $w_j$ being a word token.\n","* $\\vect{v} = \\{ v_1, \\ldots, v_V\\}$: A vocabulary, each element $v_k$ being a word type.\n","* $\\vect{x} = \\langle x_1, \\ldots, x_X \\rangle$: Input features to a model.\n","* $\\vect{y} = \\{ y_1, \\ldots, y_N \\}$: The output classes of a model, each element $y_i$ being a class label.\n","* $\\vect{T} = \\langle \\vect{w}^{(1)}, \\ldots, \\vect{w}^{(T)} \\rangle$: The training corpus of texts.\n","* $\\vect{Y} = \\langle y^{(1)}, \\ldots, y^{(T)} \\rangle$: The corresponding gold labels for the training examples in $T$."]},{"cell_type":"markdown","id":"320375db","metadata":{"id":"320375db"},"source":["# To Do: Establish a majority baseline\n","\n","A simple baseline for classification tasks is to always predict the most common class. \n","Given a training set of texts $\\vect{T}$ labeled by classes $\\vect{Y}$, we classify an input text $\\vect{w} = w_1 \\cdots w_M$ as the class $y_i$ that occurs most frequently in the training data, that is, specified by\n","\n","$$ \\argmax{i} \\cnt{y_i} $$\n","\n","and thus ignoring the input entirely (!).\n","\n","**Implement the majority baseline and compute test accuracy using the starter code below.** For this baseline, and for the naive Bayes classifier later, we don't need to use the validation set since we don't tune any hyper-parameters."]},{"cell_type":"code","execution_count":null,"id":"a391dbd4","metadata":{"id":"a391dbd4"},"outputs":[],"source":["# TODO\n","def majority_baseline_accuracy(train_data, test_data):\n","    \"\"\"Returns the most common label in the training set, and the accuracy of\n","      the majority baseline on the test set.\n","    \"\"\"\n","    label_counts = {}\n","    for example in train_data:\n","        label = example['label']\n","        if label in label_counts:\n","            label_counts[label] += 1\n","        else:\n","            label_counts[label] = 1\n","    most_common_label = max(label_counts, key=label_counts.get)\n","    correct_predictions = 0\n","    total_predictions = 0\n","    for example in test_data:\n","        label = example['label']\n","        if label == most_common_label:\n","            correct_predictions += 1\n","        total_predictions += 1\n","    test_accuracy = correct_predictions / total_predictions\n","    return most_common_label, test_accuracy"]},{"cell_type":"markdown","id":"70be71e8","metadata":{"id":"70be71e8"},"source":["How well does your classifier work? Let's see:"]},{"cell_type":"code","execution_count":null,"id":"bc0dfd55","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bc0dfd55","executionInfo":{"status":"ok","timestamp":1683429144545,"user_tz":-180,"elapsed":1826,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"e3b27c29-1f58-44dd-c7ab-d6d153258ab3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Most common label: flight_id\n","Test accuracy:     0.683\n"]}],"source":["# Call the method to establish a baseline\n","most_common_label, test_accuracy = majority_baseline_accuracy(train_data, test_data)\n","\n","print(f'Most common label: {most_common_label}\\n'\n","      f'Test accuracy:     {test_accuracy:.3f}')"]},{"cell_type":"markdown","id":"dc3cdbfe","metadata":{"id":"dc3cdbfe"},"source":["# To Do: Implement a Naive Bayes classifier\n","\n","\n","## Review of the naive Bayes method\n","\n","Recall from lab 1-3 that the Naive Bayes classification method classifies a text $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$ as the class $y_i$ given by the following maximization:\n","\n","$$\n","\\argmax{i} \\Prob(y_i \\given \\vect{w}) \\approx \\argmax{i} \\Prob(y_i) \\cdot \\prod_{j=1}^M \\Prob(w_j \\given y_i)\n","$$\n","\n","or equivalently (since taking the log is monotonic)\n","\n","\\begin{align}\n","\\argmax{i} \\Prob(y_i \\given \\vect{w}) &= \\argmax{i} \\log\\Prob(y_i \\given \\vect{w}) \\\\\n","&\\approx \\argmax{i} \\left(\\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)\\right)\n","\\end{align}\n","\n","All we need, then, to apply the Naive Bayes classification method is values for the various log probabilities: the priors $\\log\\Prob(y_i)$ and the likelihoods $\\log\\Prob(w_j \\given y_i)$, for each feature (word) $w_j$ and each class $y_i$.\n","\n","We can estimate the prior probabilities $\\Prob(y_i)$ by examining the empirical probability in the training set. That is, we estimate \n","\n","$$ \\Prob(y_i) \\approx \\frac{\\cnt{y_i}}{\\sum_j \\cnt{y_j}} $$\n","\n","We can estimate the likelihood probabilities $\\Prob(w_j \\given y_i)$ similarly by examining the empirical probability in the training set. That is, we estimate \n","\n","$$ \\Prob(w_j \\given y_i) \\approx \\frac{\\cnt{w_j, y_i}}{\\sum_{j'} \\cnt{w_{j'}, y_i}} $$\n","\n","To allow for cases in which the count $\\cnt{w_j, y_i}$ is zero, we can use a modified estimate incorporating add-$\\delta$ smoothing:\n","\n","$$ \\Prob(w_j \\given y_i) \\approx \\frac{\\cnt{w_j, y_i} + \\delta}{\\sum_{j'} \\cnt{w_{j'}, y_i} + \\delta \\cdot V} $$"]},{"cell_type":"markdown","id":"f02442b3","metadata":{"id":"f02442b3"},"source":["## Two conceptions of the naive Bayes method implementation\n","\n","We can store all of these parameters in different ways, leading to two different implementation conceptions. We review two conceptions of implementing the naive Bayes classification of a text $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$, corresponding to using different representations of the input $\\vect{x}$ to the model: the index representation and the bag-of-words representation. \n","\n","Within each conception, the parameters of the model will be stored in one or more matrices. The conception dictates what operations will be performed with these matrices.\n","\n","### Using the index representation\n","\n","In the first conception, we take the input elements $\\vect{x} = \\langle x_1, x_2, \\ldots, x_M \\rangle$ to be the _vocabulary indices_ of the words $\\vect{w} = w_1 \\cdots w_M$. That is, each word token $w_i$ is of the word type in the vocabulary $\\vect{v}$ at index $x_i$, so \n","\n","$$ v_{x_i} = w_i $$\n","\n","In this representation, the input vector has the same length as the word sequence.\n","\n","We think of the likelihood probabilities as forming a matrix, call it $\\vect{L}$, where the $i,j$-th element stores $\\log \\Prob(v_j \\given y_i)$. \n","\n","$$\\vect{L}_{ij} = \\log\\Prob(v_j \\given y_i)$$\n","\n","Similarly, for the priors, we'll have \n","\n","$$\\vect{P}_{i} = \\log\\Prob(y_i)$$\n","\n","Now the maximization can be implemented as \n","\n","\\begin{align}\n","\\argmax{i} \\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)\n","&= \\argmax{i} \\vect{P}_i + \\sum_{j=1}^M \\vect{L}_{i, x_j}\n","\\end{align}\n","\n","Implemented in this way, we see that the use of each input $x_i$ is as an _index_ into the likelihood matrix. \n","\n","### Using the bag-of-words representation\n","\n","<img src=\"https://github.com/nlp-course/data/raw/master/Resources/naive-bayes-figure.png\" width=400 align=right />\n","\n","Notice that since each word in the input is treated separately, the order of the words doesn't matter. Rather, all that matters is how frequently each word type occurs in a text. Consequently, we can use the bag-of-words representation introduced in lab 1-1.\n","\n","Recall that the bag-of-words representation of a text is just its frequency distribution over the vocabulary, which we will notate $bow(\\vect{w})$. Given a vocabulary of word types $\\vect{v} = \\langle v_1, v_2, \\ldots, v_V \\rangle$, the representation of a sentence $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$ is a vector $\\vect{x}$ of size $V$, where \n","\n","$$\\begin{aligned}\n","bow(\\vect{w})_j &= \\sum_{i=1}^M 1[w_i = v_j] & \\mbox{for $1 \\leq j \\leq V$}\n","\\end{aligned}$$\n","\n","We write $1[w_i = v_j]$ to indicate 1 if $w_i = v_j$ and 0 otherwise. For convenience, we'll add an extra $(V+1)$-st element to the end of the bag-of-words vector, a single $1$ whose use will be clear shortly. That is,\n","\n","$$bow(\\vect{w})_{V+1} = 1$$\n","\n","Under this conception, then, we'll take the input $\\vect{x}$ to be $bow(\\vect{w})$. Instead of the input having the same length as the text, it has the same length as the vocabulary.\n","\n","As described in lecture, represented in this way, the quantity to be maximized in the naive Bayes method\n","\n","$$\\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)$$\n","\n","can be calculated as \n","\n","$$\\log\\Prob(y_i) + \\sum_{j=1}^V x_j \\cdot \\log\\Prob(v_j \\given y_i)$$\n","\n","which is just $\\vect{U} \\vect{x}$ for a suitable choice of $N \\times (V+1)$ matrix $\\vect{U}$, namely\n","\n","$$ \\vect{U}_{ij} = \\left\\{\n","    \\begin{array}{ll}\n","        \\log \\Prob(v_j \\given y_i) & \\mbox{$1 \\leq i \\leq N$ and $1 \\leq j \\leq V$} \\\\\n","        \\log \\Prob(y_i) & \\mbox{$1 \\leq i \\leq N$ and $j = V+1$} \n","    \\end{array} \\right.\n","$$\n","\n","Under this implementation conception, we've reduced naive Bayes calculations to a single matrix operation. This conception is depicted in the figure at right.\n","\n","You are free to use either conception in your implementation of naive Bayes."]},{"cell_type":"markdown","id":"87aad4d7","metadata":{"id":"87aad4d7"},"source":["## Implement the naive Bayes classifier\n"," \n","For the implementation, we ask you to implement a Python class `NaiveBayes` that will have (at least) the following three methods:\n","\n","1. `__init__`: An initializer that takes `text_vocab`, `label_vocab`, and `pad_index` as inputs.\n","\n","2. `train`: A method that takes a training data iterator and estimates all of the log probabilities $\\log\\Prob(y_i)$ and $\\log\\Prob(v_j \\given y_i)$ as described above. Perform add-$\\delta$ smoothing with $\\delta=1$. These parameters will be used by the `evaluate` method to evaluate a test dataset for accuracy, so you'll want to store them in some data structures in objects of the class.\n","\n","3. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n","\n","You can organize your code using either of the conceptions of Naive Bayes described above.\n","\n","You should expect to achieve about an **86% test accuracy** on the ATIS task."]},{"cell_type":"code","source":["import numpy as np\n","\n","class NaiveBayes:\n","    def __init__(self, text_vocab, label_vocab, pad_index):\n","        self.text_vocab = text_vocab\n","        self.label_vocab = label_vocab\n","        self.V = len(text_vocab) # vocabulary size\n","        self.N = len(label_vocab) # the number of classes\n","        self.pad_index = pad_index\n","        self.log_prior = None\n","        self.log_likelihood = None\n","        self.delta=1.0\n","    def train(self, train_iterator):\n","        num_labels = len(self.label_vocab)\n","        num_words = len(self.text_vocab)\n","       \n","        # Initialize count dictionaries for prior and likelihood\n","        label_counts = np.zeros(self.N) # count of examples for each class\n","        text_counts = np.zeros((self.N, self.V)) # count of occurrences of each word type for each class\n","\n","        # Count occurrences of labels and words in the training data\n","        for batch in train_iterator:\n","            texts, labels = batch['input_ids'], batch['label_ids']\n","            \n","            for label in labels:\n","                label_int = label.item()\n","                if 0 <= label_int < num_labels:\n","                    label_counts[label_int] += 1\n","\n","                 \n","              # Text count: count occurrences of each word type for each class\n","            for label, text in zip(labels, texts):\n","                  label_int = label.item()\n","                  text_vec = text.cpu().numpy()\n","                  for word_idx in text_vec:\n","                      if 0 <= word_idx < self.V:\n","                          text_counts[label_int, word_idx] += 1\n","\n","        # Calculate log prior probabilities\n","        self.log_prior = np.log(label_counts / np.sum(label_counts))\n","\n","        # Calculate log likelihood probabilities with smoothing\n","        denom = np.sum(text_counts, axis=1, keepdims=True)\n","        self.log_likelihood = np.log((text_counts + self.delta) / (denom + self.delta * self.V))\n","                  \n","        # Remove any NaN values caused by taking the log of zero probabilities\n","        self.log_likelihood[np.isnan(self.log_likelihood)] = -np.inf\n","                  \n","    def evaluate(self, test_data):\n","        num_correct = 0\n","        total = 0\n","\n","        for batch in test_data:\n","            sentences, true_labels = batch['input_ids'], batch['label_ids']\n","            #print(\"TRUE\",true_labels.cpu().numpy())\n","            predicted_labels = self.classify_batch(sentences)\n","            #print(\"predicted\",predicted_labels)\n","            num_correct += np.sum(predicted_labels == true_labels.cpu().numpy())\n","            total += len(true_labels)\n","\n","        accuracy = num_correct / total\n","        return accuracy\n","    def classify_batch(self, sentences):\n","        batch_size = sentences.shape[0]\n","        bow = np.zeros((batch_size, self.V), dtype=int)\n","\n","        # Reverse the text_vocab dictionary\n","        reversed_vocab = {v: k for k, v in self.text_vocab.items()}\n","        for i, sentence in enumerate(sentences):\n","         sentence_vec=sentence.cpu().numpy()\n","         for word_id in sentence_vec:\n","            if word_id != self.pad_index:\n","                word = reversed_vocab.get(word_id)\n","                if word is not None:\n","                    word_idx = self.text_vocab[word]\n","                    bow[i, word_idx] += 1\n","\n","           \n","        \n","        scores = np.dot(bow, self.log_likelihood.T) + self.log_prior\n","        predicted_label_indices = np.argmax(scores, axis=1)\n","        \n","        return predicted_label_indices\n","\n","\n"],"metadata":{"id":"WiI40gofPmgz","executionInfo":{"status":"ok","timestamp":1683432509852,"user_tz":-180,"elapsed":1042,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"id":"WiI40gofPmgz","execution_count":87,"outputs":[]},{"cell_type":"code","execution_count":88,"id":"612fa845","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"612fa845","executionInfo":{"status":"ok","timestamp":1683432512060,"user_tz":-180,"elapsed":1706,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"54c0d5a9-bc62-4534-a4ae-edc5dd320f07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training accuracy: 0.858\n","Test accuracy:     0.835\n"]}],"source":["# Instantiate and train classifier\n","nb_classifier = NaiveBayes(text_vocab, label_vocab, pad_index)\n","nb_classifier.train(train_iter)\n","\n","# Evaluate model performance\n","print(f'Training accuracy: {nb_classifier.evaluate(train_iter):.3f}\\n'\n","      f'Test accuracy:     {nb_classifier.evaluate(test_iter):.3f}')"]},{"cell_type":"markdown","id":"f7b64b4f","metadata":{"id":"f7b64b4f"},"source":["# To Do: Implement a logistic regression classifier\n","\n","In this part, you'll complete a PyTorch implementation of a logistic regression (equivalently, a single layer perceptron) classifier. We review logistic regression here highlighting the similarities to the matrix-multiplication conception of naive Bayes. Thus, we take the input $\\vect{x}$ to be the bag-of-words representation $bow(\\vect{w})$. But as before you are free to use either implementation approach.\n","\n","## Review of logistic regression\n","\n","Similar to naive Bayes, in logistic regression, we assign a probability to a text $\\vect{x}$ by merely multiplying an $N \\times V$ matrix $\\vect{U}$ by it. However, we don't stipulate that the values in the matrix $\\vect{U}$ be estimated from the training corpus in the \"naive Bayes\" manner. Instead, we allow them to take on any value, using a training regime to select good values.\n","\n","In order to make sure that the output of the matrix multiplication $\\vect{U}\\vect{x}$ is mapped onto a probability distribution, we apply a nonlinear function to renormalize the values. We use the softmax function, a generalization of the sigmoid function from lab 1-4, defined by \n","\n","$$\\softmax(\\vect{z})_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{N} \\exp(z_j)}$$\n","\n","for each of the indices $i$ from $1$ to $N$.\n","\n","In summary, we model $\\Prob (y \\given \\vect{x})$ as\n","\n","$$ \\Prob(y_i \\given \\vect{x}) = \\softmax ( \\vect{U} \\vect{x} )_i $$\n","\n","<img src=\"https://github.com/nlp-course/data/raw/master/Resources/logistic-regression-figure.png\" alt=\"logistic regression illustration\" width=\"400\"  align=right />\n","\n","The calculation of $\\Prob(y \\given \\vect{x})$ for each text $\\vect{x}$ is referred to as the _forward_ computation. In summary, the forward computation for logistic regression involves a linear calculation ($\\vect{U} \\vect{x}$) followed by a nonlinear calculation ($\\softmax$). We think of the perceptron (and more generally many of these neural network models) as transforming from one representation to another. A perceptron performs a linear transformation from the index or bag-of-words representation of the text to a representation as a vector, followed by a nonlinear transformation, a softmax or sigmoid, giving a representation as a probability distribution over the class labels. This single-layer perceptron thus involves two _sublayers_. (In the next part of the project segment, you'll experiment with a multilayer perceptron, with two perceptron layers, and hence four sublayers.)\n","\n","The loss function you'll use is the negative log probability $-\\log \\Prob (y \\given \\vect{x})$. The negative is used, since it is convention to minimize loss, whereas we want to maximize log likelihood. \n","\n","The forward and loss computations are illustrated in the figure at right. In practice, for numerical stability reasons, PyTorch absorbs the softmax operation into the loss function `nn.CrossEntropyLoss`. That is, the input to the `nn.CrossEntropyLoss` function is the vector of sums $\\vect{U} \\vect{x}$ (the last step in the box marked \"your job\" in the figure) rather than the vector of probabilities $\\Prob(y \\given \\vect{x})$. That makes things easier for you (!), since you're responsible only for the first sublayer.\n","\n","Given a forward computation, the weights can then be adjusted by taking a step opposite to the gradient of the loss function. Adjusting the weights in this way is referred to as the _backward_ computation. Fortunately, `torch` takes care of the backward computation for you, just as in lab 1-5.\n","\n","The optimization process of performing the forward computation, calculating the loss, and performing the backward computation to improve the weights is done repeatedly until the process converges on a (hopefully) good set of weights. You'll find this optimization process in the `train_all` method that we've provided. The trained weights can then be used to perform classification on a test set. See the `evaluate` method."]},{"cell_type":"markdown","id":"5557c865","metadata":{"id":"5557c865"},"source":["## Implement the logistic regression classifier\n","\n","For the implementation, we ask you to implement a logistic regression classifier as a subclass of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module). You need to implement the following methods:\n","\n","1. `__init__`: an initializer that takes `text_vocab`, `label_vocab`, and `pad_index` as inputs.\n","\n","    During initialization, you'll want to define a [tensor](https://pytorch.org/docs/stable/tensors.html#torch-tensor) of weights, wrapped in [`torch.nn.Parameter`](https://pytorch.org/docs/master/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter), [initialized randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_), which plays the role of $\\vect{U}$. The elements of this tensor are the parameters of the `torch.nn` instance in the following special technical sense: It is the parameters of the module whose gradients will be calculated and whose values will be updated. Alternatively, **you might find it easier** to use the [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) which is a wrapper to the weight tensor with a lookup implementation.\n","\n","2. `forward`: given a text batch of size `batch_size X max_length`, return a tensor of logits of size `batch_size X num_labels`. That is, for each text $\\vect{x}$ in the batch and each label $y$, you'll be calculating $\\vect{U}\\vect{x}$ as shown in the figure, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you won't need to deal with that.\n","\n","3. `train_all`: A method that performs training. You might find lab 1-5 useful.\n","\n","4. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n","\n","Some things to consider:\n","\n","1. The parameters of the model, the weights, need to be initialized properly. We suggest initializing them to some small random values. See [`torch.uniform_`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_).\n","\n","2. You'll want to make sure that padding tokens are handled properly. What should the weight be for the padding token?\n","\n","3. In extracting the proper weights to sum up, based on the word types in a sentence, we are essentially doing a lookup operation. You might find [`nn.Embedding`](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) or [`torch.gather`](https://pytorch.org/docs/stable/generated/torch.gather.html#torch-gather) useful.\n","\n","You should expect to achieve about **90%** accuracy on the ATIS classificiation task. "]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import copy\n","from tqdm import tqdm\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, text_vocab, label_vocab, pad_index):\n","        super(LogisticRegression, self).__init__()\n","        self.pad_index = pad_index\n","        self.N = len(label_vocab)  # num_classes\n","        self.V = len(text_vocab)   # vocab_size\n","\n","        # Define the weights using nn.Embedding\n","        self.embedding = nn.Embedding(self.V, self.N)\n","\n","        # Initialize the weights\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","\n","        # Define the criterion for optimization\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","        # Initialize best_model attribute\n","        self.best_model = None\n","\n","    def forward(self, text_batch):\n","        # Perform the lookup operation using nn.Embedding\n","        embedded = self.embedding(text_batch)\n","\n","        # Calculate the logits (Ux) by summing up the embeddings\n","        logits = torch.sum(embedded, dim=1)\n","\n","        return logits\n","\n","    def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n","        self.train()\n","        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","        best_validation_accuracy = -float('inf')\n","\n","        for epoch in range(epochs):\n","            c_num = 0\n","            total = 0\n","            running_loss = 0.0\n","\n","            for batch in tqdm(train_iter, desc='batch', leave=False):\n","                optimizer.zero_grad()\n","\n","                text_batch = batch['input_ids']\n","                labels = batch['label_ids']\n","\n","                # Forward pass\n","                logits = self.forward(text_batch)\n","\n","                # Compute loss\n","                loss = self.criterion(logits, labels)\n","\n","                # Backward pass and update weights\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Calculate accuracy and loss\n","                predictions = torch.argmax(logits, dim=1)\n","                total += predictions.size(0)\n","                c_num += (predictions == labels).sum().item()\n","                running_loss += loss.item() * predictions.size(0)\n","\n","            # Evaluate and track improvements on the validation dataset\n","            validation_accuracy = self.evaluate(val_iter)\n","            if validation_accuracy > best_validation_accuracy:\n","                best_validation_accuracy = validation_accuracy\n","                self.best_model = copy.deepcopy(self.state_dict())\n","\n","            epoch_loss = running_loss / total\n","            epoch_acc = c_num / total\n","            print(f\"Epoch: {epoch+1} Loss: {epoch_loss:.4f} Train Acc: {epoch_acc:.4f} Val Acc: {validation_accuracy:.4f}\")\n","\n","        self.load_state_dict(self.best_model)\n","\n","    def evaluate(self, iterator):\n","        self.eval()\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for batch in iterator:\n","                text_batch = batch['input_ids']\n","                labels = batch['label_ids']\n","\n","                # Forward pass\n","                logits = self.forward(text_batch)\n","\n","                # Calculate accuracy\n","                predictions = torch.argmax(logits, dim=1)\n","                correct += (predictions == labels).sum().item()\n","                total += labels.size(0)\n","\n","        accuracy = correct / total\n","\n","        return accuracy\n"],"metadata":{"id":"MSpxw34Hby3q","executionInfo":{"status":"ok","timestamp":1683433146277,"user_tz":-180,"elapsed":953,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"id":"MSpxw34Hby3q","execution_count":99,"outputs":[]},{"cell_type":"code","execution_count":100,"id":"bd3f32cb","metadata":{"id":"bd3f32cb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683433153307,"user_tz":-180,"elapsed":4732,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"cad5129f-abae-4d25-909d-6aaa750a4e39"},"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 Loss: 1.2993 Train Acc: 0.7077 Val Acc: 0.7454\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 Loss: 0.6839 Train Acc: 0.8150 Val Acc: 0.8004\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 3 Loss: 0.5130 Train Acc: 0.8671 Val Acc: 0.8452\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 4 Loss: 0.4130 Train Acc: 0.8924 Val Acc: 0.8676\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 5 Loss: 0.3451 Train Acc: 0.9093 Val Acc: 0.8778\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 6 Loss: 0.2954 Train Acc: 0.9214 Val Acc: 0.8921\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 7 Loss: 0.2577 Train Acc: 0.9361 Val Acc: 0.9084\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 8 Loss: 0.2281 Train Acc: 0.9443 Val Acc: 0.9145\n","Test accuracy: 0.9219\n"]}],"source":["# Instantiate the logistic regression classifier and run it\n","model = LogisticRegression(text_vocab, label_vocab, pad_index).to(device) \n","model.train_all(train_iter, val_iter)\n","model.load_state_dict(model.best_model)\n","test_accuracy = model.evaluate(test_iter)\n","print (f'Test accuracy: {test_accuracy:.4f}')"]},{"cell_type":"markdown","id":"30015b49","metadata":{"id":"30015b49"},"source":["# To Do: Implement a multilayer perceptron\n","\n","## Review of multilayer perceptrons\n","\n","<img src=\"https://github.com/nlp-course/data/raw/master/Resources/multilayer-perceptron-figure.png\" alt=\"multilayer perceptron illustration\" width=\"400\"  align=right />\n","\n","In the last part, you implemented a perceptron, a model that involved a linear calculation (the sum of weights) followed by a nonlinear calculation (the softmax, which converts the summed weight values to probabilities). In a multi-layer perceptron, we take the output of the first perceptron to be the input of a second perceptron (and of course, we could continue on with a third or even more).\n","\n","In this part, you'll implement the forward calculation of a two-layer perceptron, again letting PyTorch handle the backward calculation as well as the optimization of parameters. The first layer will involve a linear summation as before and a **sigmoid** as the nonlinear function. The second will involve a linear summation and a softmax (the latter absorbed, as before, into the loss function). Thus, the difference from the logistic regression implementation is simply the adding of the sigmoid and second linear calculations. See the figure for the structure of the computation. \n","\n"]},{"cell_type":"markdown","id":"ac2ed538","metadata":{"id":"ac2ed538"},"source":["## Implement a multilayer perceptron classifier\n","\n","For the implementation, we ask you to implement a two layer perceptron classifier, again as a subclass of the [`torch.nn` module](https://pytorch.org/docs/stable/nn.html). You might reuse quite a lot of the code from logistic regression. As before, you need to implement the following methods:\n","\n","1. `__init__`: An initializer that takes `text_vocab`, `label_vocab`, `pad_index`, and `hidden_size` specifying the size of the hidden layer (e.g., in the above illustration, `hidden_size` is `D`).\n","\n","    During initialization, you'll want to define two tensors of weights, which serve as the parameters of this model, one for each layer. You'll want to [initialize them randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_). \n","    \n","    The weights in the first layer are a kind of lookup (as in the previous part), mapping words to a vector of size `hidden_size`. The [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) is a good way to set up and make use of this weight tensor.\n","    \n","    The weights in the second layer define a linear mapping from vectors of size `hidden_size` to vectors of size `num_labels`. The [`nn.Linear` module](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) or [`torch.mm`](https://pytorch.org/docs/master/generated/torch.mm.html) for matrix multiplication may be helpful here.\n","\n","2. `forward`: Given a text batch of size `batch_size X max_length`, the `forward` function returns a tensor of logits of size `batch_size X num_labels`. \n","\n","    That is, for each text $\\vect{x}$ in the batch and each label $c$, you'll be calculating $MLP(bow(\\vect{x}))$ as shown in the illustration above, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you don't need to worry about that.\n","    \n","    For the sigmoid sublayer, you might find [`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) useful.\n","    \n","3. `train_all`: A method that performs training. You might find lab 1-5 useful.\n","\n","4. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n","\n","You should expect to achieve at least **90%** accuracy on the ATIS classificiation task. "]},{"cell_type":"code","execution_count":null,"id":"ce3b3c9d","metadata":{"id":"ce3b3c9d"},"outputs":[],"source":["class MultiLayerPerceptron(nn.Module):\n","  def __init__ (self, text_vocab, label_vocab, pad_index, hidden_size=128): \n","    super().__init__ ()\n","    self.pad_index = pad_index\n","    self.hidden_size = hidden_size\n","    # Keep the vocabulary sizes available\n","    self.N = len(label_vocab) # num_classes\n","    self.V = len(text_vocab)  # vocab_size\n","    # Specify cross-entropy loss for optimization\n","    self.criterion = nn.CrossEntropyLoss()\n","    # TODO: Create and initialize neural modules\n","    ...\n","\n","  def forward(self, text_batch):\n","    # TODO: Calculate the logits for the `text_batch`, \n","    #       returning a tensor of size batch_size x num_labels\n","    ...\n","  \n","  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n","    # Switch the module to training mode\n","    self.train()\n","    # Use Adam to optimize the parameters\n","    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","    best_validation_accuracy = -float('inf')\n","    best_model = None\n","    # Run the optimization for multiple epochs\n","    with tqdm(range(epochs), desc='train', position=0) as pbar:\n","      for epoch in pbar:\n","        c_num = 0\n","        total = 0\n","        running_loss = 0.0\n","        for batch in tqdm(train_iter, desc='batch', leave=False):\n","          # TODO: set labels, compute logits (Ux in this model), \n","          #       loss, and update parameters\n","          ...\n","          labels = ...\n","          logits = ...\n","          loss = ...\n","          ...\n","          # Prepare to compute the accuracy\n","          predictions = torch.argmax(logits, dim=1)\n","          total += predictions.size(0)\n","          c_num += (predictions == labels).float().sum().item()        \n","          running_loss += loss.item() * predictions.size(0)\n","\n","        # Evaluate and track improvements on the validation dataset\n","        validation_accuracy = self.evaluate(val_iter)\n","        if validation_accuracy > best_validation_accuracy:\n","          best_validation_accuracy = validation_accuracy\n","          self.best_model = copy.deepcopy(self.state_dict())\n","        epoch_loss = running_loss / total\n","        epoch_acc = c_num / total\n","        pbar.set_postfix(epoch=epoch+1, loss=epoch_loss, train_acc = epoch_acc, val_acc=validation_accuracy)\n","\n","  def evaluate(self, iterator):\n","    \"\"\"Returns the model's accuracy on a given dataset `iterator`.\"\"\"\n","    # TODO: Compute accuracy\n","    ..."]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import copy\n","from tqdm import tqdm\n","\n","class MultiLayerPerceptron(nn.Module):\n","    def __init__(self, text_vocab, label_vocab, pad_index, hidden_size=128):\n","        super(MultiLayerPerceptron, self).__init__()\n","        self.pad_index = pad_index\n","        self.hidden_size = hidden_size\n","        self.N = len(label_vocab)  # num_classes\n","        self.V = len(text_vocab)   # vocab_size\n","\n","        # Define the weights using nn.Embedding for the first layer\n","        self.embedding = nn.Embedding(self.V, self.hidden_size)\n","\n","        # Define the weights for the second layer\n","        self.linear = nn.Linear(self.hidden_size, self.N)\n","\n","        # Initialize the weights\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","        self.linear.weight.data.uniform_(-0.1, 0.1)\n","\n","        # Define the criterion for optimization\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","        # Initialize best_model attribute\n","        self.best_model = None\n","\n","    def forward(self, text_batch):\n","       embedded = self.embedding(text_batch)\n","       embedded = torch.sum(embedded, dim=1)\n","       sigmoid_output = torch.sigmoid(embedded)\n","       linear_output = self.linear(sigmoid_output)\n","\n","     \n","\n","       return linear_output\n","\n","    def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n","        self.train()\n","        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","        best_validation_accuracy = -float('inf')\n","\n","        for epoch in range(epochs):\n","            c_num = 0\n","            total = 0\n","            running_loss = 0.0\n","\n","            for batch in tqdm(train_iter, desc='batch', leave=False):\n","                optimizer.zero_grad()\n","\n","                text_batch = batch['input_ids']\n","                labels = batch['label_ids']\n","\n","                # Forward pass\n","                logits = self.forward(text_batch)\n","               \n","\n","                # Compute loss\n","                loss = self.criterion(logits,labels)\n","\n","                # Backward pass and update weights\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Calculate accuracy and loss\n","                predictions = torch.argmax(logits, dim=1)\n","                total += predictions.size(0)\n","                c_num += (predictions == labels).sum().item()\n","                running_loss += loss.item() * predictions.size(0)\n","\n","            # Evaluate and track improvements on the validation dataset\n","            validation_accuracy = self.evaluate(val_iter)\n","            if validation_accuracy > best_validation_accuracy:\n","                best_validation_accuracy = validation_accuracy\n","                self.best_model = copy.deepcopy(self.state_dict())\n","\n","            epoch_loss = running_loss / total\n","            epoch_acc = c_num / total\n","            print(f\"Epoch: {epoch+1} Loss: {epoch_loss:.4f} Train Acc: {epoch_acc:.4f} Val Acc: {validation_accuracy:.4f}\")\n","\n","        self.load_state_dict(self.best_model)\n","\n","    def evaluate(self, iterator):\n","        self.eval()\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for batch in iterator:\n","                text_batch = batch['input_ids']\n","                labels = batch['label_ids']\n","\n","                # Forward pass\n","                logits = self.forward(text_batch)\n","\n","                # Calculate accuracy\n","                predictions = torch.argmax(logits, dim=1)\n","                correct += (predictions == labels).sum().item()\n","                total += labels.size(0)\n","\n","        accuracy =correct/total\n","        return accuracy\n"],"metadata":{"id":"o2OOoIuKekBP","executionInfo":{"status":"ok","timestamp":1683434653369,"user_tz":-180,"elapsed":2,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}}},"id":"o2OOoIuKekBP","execution_count":126,"outputs":[]},{"cell_type":"code","execution_count":127,"id":"025db464","metadata":{"id":"025db464","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683434660125,"user_tz":-180,"elapsed":5577,"user":{"displayName":"Anushka Deshpande","userId":"10882130751688122557"}},"outputId":"f36bf03b-d4fa-4f74-ec8c-6c62ece3de31"},"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 Loss: 1.0989 Train Acc: 0.7241 Val Acc: 0.7841\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 Loss: 0.5724 Train Acc: 0.8520 Val Acc: 0.8717\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 3 Loss: 0.3824 Train Acc: 0.9048 Val Acc: 0.9002\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 4 Loss: 0.2834 Train Acc: 0.9333 Val Acc: 0.9145\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 5 Loss: 0.2208 Train Acc: 0.9486 Val Acc: 0.9267\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 6 Loss: 0.1777 Train Acc: 0.9607 Val Acc: 0.9389\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 7 Loss: 0.1466 Train Acc: 0.9657 Val Acc: 0.9470\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch: 8 Loss: 0.1234 Train Acc: 0.9710 Val Acc: 0.9450\n","Test accuracy: 0.9286\n"]}],"source":["# Instantiate classifier and run it\n","model = MultiLayerPerceptron(text_vocab, label_vocab, pad_index, hidden_size=128).to(device) \n","model.train_all(train_iter, val_iter)\n","model.load_state_dict(model.best_model)\n","test_accuracy = model.evaluate(test_iter)\n","print (f'Test accuracy: {test_accuracy:.4f}')"]},{"cell_type":"markdown","id":"9193bd65","metadata":{"deletable":false,"editable":false,"id":"9193bd65"},"source":["<!-- BEGIN QUESTION -->\n","\n","# Lessons learned\n","\n","Take a look at some of the examples that were classified correctly and incorrectly by your best method.\n","\n","**Question:** Do you notice anything about the incorrectly classified examples that might indicate _why_ they were classified incorrectly?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_lessons\n","manual: true\n","-->"]},{"cell_type":"markdown","id":"ab01d770","metadata":{"id":"ab01d770"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","id":"49170382","metadata":{"deletable":false,"editable":false,"id":"49170382"},"source":["<!-- END QUESTION -->"]},{"cell_type":"code","execution_count":null,"id":"c4eeb544","metadata":{"id":"c4eeb544"},"outputs":[],"source":["..."]},{"cell_type":"markdown","id":"c4cc72b9","metadata":{"deletable":false,"editable":false,"id":"c4cc72b9"},"source":["<!-- BEGIN QUESTION -->\n","\n","# Debrief\n","\n","**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on include the following: \n","\n","* Was the project segment clear or unclear? Which portions?\n","* Were the readings appropriate background for the project segment? \n","* Are there additions or changes you think would make the project segment better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"]},{"cell_type":"markdown","id":"4b81f183","metadata":{"id":"4b81f183"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","id":"a262220b","metadata":{"id":"a262220b"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","# Instructions for submission of the project segment\n","\n","This project segment should be submitted to Gradescope at <https://rebrand.ly/project1-submit-code> and <https://rebrand.ly/project1-submit-pdf>, which will be made available some time before the due date.\n","\n","Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion. You should submit your code to Gradescope at the code submission assignment at <https://rebrand.ly/project1-submit-code>.\n","\n","We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope at <https://rebrand.ly/project1-submit-pdf>."]},{"cell_type":"markdown","id":"cc96d77a","metadata":{"id":"cc96d77a"},"source":["# End of project segment 1"]}],"metadata":{"accelerator":"GPU","celltoolbar":"Edit Metadata","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"otter-latest","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"title":"CS187 Project Segment 1: Text Classification","vscode":{"interpreter":{"hash":"4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"6b31c1d9ba4d4b599dd0a258e3f443a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_442ed2e84c534a529ae79a98334a7cb3","IPY_MODEL_0c862ced84bb4fc5ac665b08a69154c7","IPY_MODEL_a512dfeda574445cad455b796a0e6e80"],"layout":"IPY_MODEL_253922cc8a09457694f7a7d98317650b"}},"442ed2e84c534a529ae79a98334a7cb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f77de170a10484e9f8dab918d15c17e","placeholder":"​","style":"IPY_MODEL_79c610c0cf9b4375ac4fe8528c9a69bc","value":"Downloading data files: 100%"}},"0c862ced84bb4fc5ac665b08a69154c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ea0d180721f489eb10daa77a8ddf78c","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f6ebc1e599b41fe943bc11988ce87f1","value":3}},"a512dfeda574445cad455b796a0e6e80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e92c5f91d41845f7adc5a9331c707530","placeholder":"​","style":"IPY_MODEL_a181c889744d4a20863f3c40f7c17647","value":" 3/3 [00:00&lt;00:00, 130.64it/s]"}},"253922cc8a09457694f7a7d98317650b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f77de170a10484e9f8dab918d15c17e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79c610c0cf9b4375ac4fe8528c9a69bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ea0d180721f489eb10daa77a8ddf78c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f6ebc1e599b41fe943bc11988ce87f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e92c5f91d41845f7adc5a9331c707530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a181c889744d4a20863f3c40f7c17647":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c614e8add25e4c87a41250217b930d0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3837a9f2be294fb690eb9dd4cbcdbc9e","IPY_MODEL_743add99ef7e4092b45d848792dada4a","IPY_MODEL_e77723ad8c1843e6aa6c8c12cf3d27f4"],"layout":"IPY_MODEL_5ffe96baa832488d8377f8f283b5b56c"}},"3837a9f2be294fb690eb9dd4cbcdbc9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e4ec5ffac3a4362b8f4fd68301526c2","placeholder":"​","style":"IPY_MODEL_b1af7b86c42148a685e2b810aea4013a","value":"Extracting data files: 100%"}},"743add99ef7e4092b45d848792dada4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41599c8908d246d3b59e91cb4767572c","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41c7618482f047ef8effda0ea2ae41b6","value":3}},"e77723ad8c1843e6aa6c8c12cf3d27f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e497cc46ab9455ba59e4b58d97bf187","placeholder":"​","style":"IPY_MODEL_22a2122d70194653ab053878b7004a93","value":" 3/3 [00:00&lt;00:00, 132.18it/s]"}},"5ffe96baa832488d8377f8f283b5b56c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e4ec5ffac3a4362b8f4fd68301526c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1af7b86c42148a685e2b810aea4013a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41599c8908d246d3b59e91cb4767572c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41c7618482f047ef8effda0ea2ae41b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e497cc46ab9455ba59e4b58d97bf187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22a2122d70194653ab053878b7004a93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4ebea9c16584fa7ac0194dd32314669":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3a6858ff7f046a08ce496c1ef4594d6","IPY_MODEL_28ab661c633c4f9bb54aac1923f9e6a3","IPY_MODEL_252ded762b9d4071a978fc11f55c0bf7"],"layout":"IPY_MODEL_d0108b78398941098e4f32cf362cc842"}},"d3a6858ff7f046a08ce496c1ef4594d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_940c2fd105394735917455fb08ed3eff","placeholder":"​","style":"IPY_MODEL_d16398f1672b42d182582a919a646033","value":"Generating train split: "}},"28ab661c633c4f9bb54aac1923f9e6a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f47bec29de343348e3a7b8d85110544","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_740f90f1cf95466a822b6e1746149c30","value":1}},"252ded762b9d4071a978fc11f55c0bf7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44d1fd1128034ae09e2f3af5d3e8def4","placeholder":"​","style":"IPY_MODEL_8c56f7e6482e45ac9f4fb3e028ca5066","value":" 0/0 [00:00&lt;?, ? examples/s]"}},"d0108b78398941098e4f32cf362cc842":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"940c2fd105394735917455fb08ed3eff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d16398f1672b42d182582a919a646033":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f47bec29de343348e3a7b8d85110544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"740f90f1cf95466a822b6e1746149c30":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44d1fd1128034ae09e2f3af5d3e8def4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c56f7e6482e45ac9f4fb3e028ca5066":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b195f2b9906a489c935bcf2c50949dd7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5158791422b4b92802a30aa11e6781c","IPY_MODEL_c8cd2be8f9b04b87854189609bb6e8e1","IPY_MODEL_a096eaa18ccb41b89ee8ef0f3dc6a465"],"layout":"IPY_MODEL_8c7e3d632594412fa7eb0dbcdc8c3c6c"}},"a5158791422b4b92802a30aa11e6781c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c9510f4b92a4e0bac46e6ac1b70b06c","placeholder":"​","style":"IPY_MODEL_bfc24ac767ef4c5c8ab72b2b5131d67f","value":"Generating val split: "}},"c8cd2be8f9b04b87854189609bb6e8e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d4bbff2d2e34a1ab79880f7b78d1dd6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1b84104b302437d92ee5e841e85d44f","value":1}},"a096eaa18ccb41b89ee8ef0f3dc6a465":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fa10e0214724d789719cfd0b55fbb39","placeholder":"​","style":"IPY_MODEL_acf06df9ffae4eb992fbb62aa38d9d72","value":" 0/0 [00:00&lt;?, ? examples/s]"}},"8c7e3d632594412fa7eb0dbcdc8c3c6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"9c9510f4b92a4e0bac46e6ac1b70b06c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfc24ac767ef4c5c8ab72b2b5131d67f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d4bbff2d2e34a1ab79880f7b78d1dd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"b1b84104b302437d92ee5e841e85d44f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4fa10e0214724d789719cfd0b55fbb39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acf06df9ffae4eb992fbb62aa38d9d72":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7de46e69049a4438a7cc20285097dbae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b155d9ee8e034f4a96fd27ed966c45a5","IPY_MODEL_ba0db8f6975b4f02a7a281d6bc1304d4","IPY_MODEL_f8115349e88b4ad39f04fc04111dc2f7"],"layout":"IPY_MODEL_5765c609aaf9455095efe1e3a9c57da3"}},"b155d9ee8e034f4a96fd27ed966c45a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83c2ce70cf1f41358d87d36b5927c062","placeholder":"​","style":"IPY_MODEL_cf4bfb9c99c44ffc94aeddae15699e1c","value":"Generating test split: "}},"ba0db8f6975b4f02a7a281d6bc1304d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a1e8baf68964500a47faca8d9129346","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ece0ec209ac146cf8449ab8c128b6d24","value":1}},"f8115349e88b4ad39f04fc04111dc2f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05b7bb6048374287979886a4217738dc","placeholder":"​","style":"IPY_MODEL_ca503967579340ba90e2ec59df2b9952","value":" 0/0 [00:00&lt;?, ? examples/s]"}},"5765c609aaf9455095efe1e3a9c57da3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"83c2ce70cf1f41358d87d36b5927c062":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf4bfb9c99c44ffc94aeddae15699e1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a1e8baf68964500a47faca8d9129346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ece0ec209ac146cf8449ab8c128b6d24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05b7bb6048374287979886a4217738dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca503967579340ba90e2ec59df2b9952":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdfb94c92945459c852bed7f0b6e48ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d76d0ce0f14f40fba45044585530d538","IPY_MODEL_962b6affc164451580ae95efc8dc41c4","IPY_MODEL_588deb16d373427a8f2930296b2a80b4"],"layout":"IPY_MODEL_ef87c7678ee340659082590fd7e2d575"}},"d76d0ce0f14f40fba45044585530d538":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33d8f24e6068414781b76fbe749987f7","placeholder":"​","style":"IPY_MODEL_6f862421bd1c404ba3ea3247afe25a03","value":"100%"}},"962b6affc164451580ae95efc8dc41c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a7b645163584d0ba11aa85479ca5158","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08093e96fbb94fb08083b939ae0a9075","value":3}},"588deb16d373427a8f2930296b2a80b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90bb6be83a9d404e8601f5c423f431f5","placeholder":"​","style":"IPY_MODEL_d911dd7309344cfc8c33af93797cf30f","value":" 3/3 [00:00&lt;00:00, 105.85it/s]"}},"ef87c7678ee340659082590fd7e2d575":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33d8f24e6068414781b76fbe749987f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f862421bd1c404ba3ea3247afe25a03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a7b645163584d0ba11aa85479ca5158":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08093e96fbb94fb08083b939ae0a9075":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"90bb6be83a9d404e8601f5c423f431f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d911dd7309344cfc8c33af93797cf30f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"238a73d0a964436c97d26b523ac0fbbd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce926881a4914c0baae64d7104c9a6b6","IPY_MODEL_560ada30dd604f1596e7a37bdd32f42a","IPY_MODEL_6a8a258a1f2a4562b7a5e6cb1f560c47"],"layout":"IPY_MODEL_1b5fb14e522645848f46774a73a49c63"}},"ce926881a4914c0baae64d7104c9a6b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06ebd8677bf9432fa3456100b8ed109f","placeholder":"​","style":"IPY_MODEL_711fe84328d7475fb63f7612c874ac48","value":"Map:  91%"}},"560ada30dd604f1596e7a37bdd32f42a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f252a2ae2594ed28003d2732af4fa7f","max":4379,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d63eb0a0df4432589a0db8a8f0c253a","value":4379}},"6a8a258a1f2a4562b7a5e6cb1f560c47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca58c5af24e842be8796152ca5392f63","placeholder":"​","style":"IPY_MODEL_612b8a71634946c69b434087b16e9068","value":" 4000/4379 [00:00&lt;00:00, 4969.51 examples/s]"}},"1b5fb14e522645848f46774a73a49c63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"06ebd8677bf9432fa3456100b8ed109f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"711fe84328d7475fb63f7612c874ac48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f252a2ae2594ed28003d2732af4fa7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d63eb0a0df4432589a0db8a8f0c253a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca58c5af24e842be8796152ca5392f63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"612b8a71634946c69b434087b16e9068":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f9e92bd5c304383871caf826a57997e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b252ad85fe814531a35d37779c4a093f","IPY_MODEL_d29a50fd3eee4f039cb43620108f6639","IPY_MODEL_8da4e9f56dfc46248892be262b9e1dbc"],"layout":"IPY_MODEL_4dd74845659242fab2255df6eccada2d"}},"b252ad85fe814531a35d37779c4a093f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43f6be5e3bc142628405b0389fbcf1d4","placeholder":"​","style":"IPY_MODEL_0a04dd213d624da3bb616a7e25439622","value":"Map: 100%"}},"d29a50fd3eee4f039cb43620108f6639":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bb5dffd6e4b42ff89301ff1594e47dc","max":491,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93fa46f749184ef3812eacd6a362bb85","value":491}},"8da4e9f56dfc46248892be262b9e1dbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e6f5ea6f9634b6aac26c2a5a5d313dd","placeholder":"​","style":"IPY_MODEL_4d616e684f8944e3919f1af38b5a0b22","value":" 491/491 [00:00&lt;00:00, 4759.08 examples/s]"}},"4dd74845659242fab2255df6eccada2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"43f6be5e3bc142628405b0389fbcf1d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a04dd213d624da3bb616a7e25439622":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bb5dffd6e4b42ff89301ff1594e47dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93fa46f749184ef3812eacd6a362bb85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e6f5ea6f9634b6aac26c2a5a5d313dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d616e684f8944e3919f1af38b5a0b22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"982fc37a0ef94033b7241d9d6dd53fcc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96de8c4c1f474d55bf530f4932646f6c","IPY_MODEL_94df51c894df42ba8739394114de8399","IPY_MODEL_fdde3081c3464bb4994fbe13676dbc1e"],"layout":"IPY_MODEL_1534ef26d5a1455b9ada1f72b14191f5"}},"96de8c4c1f474d55bf530f4932646f6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcd1f3a835334bcb8d97645b365e7c45","placeholder":"​","style":"IPY_MODEL_17c7afd026564bab91af26d282dd8586","value":"Map:   0%"}},"94df51c894df42ba8739394114de8399":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_403e5d980b2a41fd88c7b72e8d9ce570","max":448,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bbf1414a2d04656a9ac4fa7d9283905","value":448}},"fdde3081c3464bb4994fbe13676dbc1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1d982d35297489885c680347ed27c00","placeholder":"​","style":"IPY_MODEL_23f0d9eb62e249fba72e81f1711c94e4","value":" 0/448 [00:00&lt;?, ? examples/s]"}},"1534ef26d5a1455b9ada1f72b14191f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"bcd1f3a835334bcb8d97645b365e7c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17c7afd026564bab91af26d282dd8586":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"403e5d980b2a41fd88c7b72e8d9ce570":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bbf1414a2d04656a9ac4fa7d9283905":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1d982d35297489885c680347ed27c00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23f0d9eb62e249fba72e81f1711c94e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8188158a16b94fb895dfc24760b0d77f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a816269269f54715b57dca731c2b491a","IPY_MODEL_5861ab1504344814943bc733d0777492","IPY_MODEL_30942ddf3b1247eda624557f9b6900ba"],"layout":"IPY_MODEL_2b8ec7cf73ac4955b7b982b2c11f506b"}},"a816269269f54715b57dca731c2b491a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea904870136b4a939110c33fa217e6d6","placeholder":"​","style":"IPY_MODEL_653b82686f9540e9a96d45f090f8b341","value":"Casting to class labels:   0%"}},"5861ab1504344814943bc733d0777492":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f7ab527cc3642ba889d7d0bf7c2ea65","max":4379,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19a018421e3e42ea9e2bcaa29fb16c68","value":4379}},"30942ddf3b1247eda624557f9b6900ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_822084aa16e947f9815b3933c29925ad","placeholder":"​","style":"IPY_MODEL_a897114130a847be92ea5ed7e2ba604d","value":" 0/4379 [00:00&lt;?, ? examples/s]"}},"2b8ec7cf73ac4955b7b982b2c11f506b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ea904870136b4a939110c33fa217e6d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"653b82686f9540e9a96d45f090f8b341":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f7ab527cc3642ba889d7d0bf7c2ea65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19a018421e3e42ea9e2bcaa29fb16c68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"822084aa16e947f9815b3933c29925ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a897114130a847be92ea5ed7e2ba604d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93e566573e4247458126803d58ec5541":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e4eaf5eb59b47b2b38717806e1e13d1","IPY_MODEL_ebcd884284d04defb93715fb4993d19b","IPY_MODEL_eb83da846708445baee136f8fecfc55a"],"layout":"IPY_MODEL_d8785f811d284561a65f7a2915da284e"}},"4e4eaf5eb59b47b2b38717806e1e13d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_694e364f094747b98255504015f90a82","placeholder":"​","style":"IPY_MODEL_4811f369c2e746e98d856aee9c41b193","value":"Casting to class labels:   0%"}},"ebcd884284d04defb93715fb4993d19b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_85e293b33a0f47a4bfab2f381f50d12d","max":491,"min":0,"orientation":"horizontal","style":"IPY_MODEL_381fb5a103bc4120b1185383e8b65b70","value":491}},"eb83da846708445baee136f8fecfc55a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e4460c5f9ee424c818ad72cb3d241b2","placeholder":"​","style":"IPY_MODEL_dd5f387430fe46ce983854e5fe0969de","value":" 0/491 [00:00&lt;?, ? examples/s]"}},"d8785f811d284561a65f7a2915da284e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"694e364f094747b98255504015f90a82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4811f369c2e746e98d856aee9c41b193":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85e293b33a0f47a4bfab2f381f50d12d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"381fb5a103bc4120b1185383e8b65b70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e4460c5f9ee424c818ad72cb3d241b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd5f387430fe46ce983854e5fe0969de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10b61ed346f84e1cac668c4460aeed05":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f0cf899c7b44fdaae425030781cf102","IPY_MODEL_b78122fda9a74554b493579e9710ff29","IPY_MODEL_74518cc52d474ddb926e3b8f918bb68c"],"layout":"IPY_MODEL_74b6e6551ab043a891ce53c53ae16aa6"}},"9f0cf899c7b44fdaae425030781cf102":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61da12044fe846d2a45bc87a1d664909","placeholder":"​","style":"IPY_MODEL_60cd8f08b7204bd5aeb0eb157dc63c10","value":"Aligning the labels:   0%"}},"b78122fda9a74554b493579e9710ff29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f5ca0f7c9af4ac89e65416ac2f7650a","max":491,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3a7314b798e42b99645f55066bd81d6","value":491}},"74518cc52d474ddb926e3b8f918bb68c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a098411a668340c884acc5d048b95dd3","placeholder":"​","style":"IPY_MODEL_a2fb9dbc8eaa4edc8479400450e721cb","value":" 0/491 [00:00&lt;?, ? examples/s]"}},"74b6e6551ab043a891ce53c53ae16aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"61da12044fe846d2a45bc87a1d664909":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60cd8f08b7204bd5aeb0eb157dc63c10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f5ca0f7c9af4ac89e65416ac2f7650a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3a7314b798e42b99645f55066bd81d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a098411a668340c884acc5d048b95dd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2fb9dbc8eaa4edc8479400450e721cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c764ff7168c74ab2b52b90dd026073c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc558a80958b4510bb07451b0262b623","IPY_MODEL_974dd5f5627e4312be489db18df40429","IPY_MODEL_a34dbed3c01741c1836217d608456134"],"layout":"IPY_MODEL_aab7b83a08c542a8b9c2c6a7361c6fd1"}},"bc558a80958b4510bb07451b0262b623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7a6a9e6881541bc963072bdd286310b","placeholder":"​","style":"IPY_MODEL_41375e74f98b42ecb1bf856be1351b52","value":"Casting to class labels:   0%"}},"974dd5f5627e4312be489db18df40429":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_accb31441c2e4946b13baf02498a3b20","max":448,"min":0,"orientation":"horizontal","style":"IPY_MODEL_246e1debf56a4f2baed2cab5bc741fd6","value":448}},"a34dbed3c01741c1836217d608456134":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_362d044fd30b47a5bc9b1e29d312cbcb","placeholder":"​","style":"IPY_MODEL_720e34f67cf04964b6aa232f56799511","value":" 0/448 [00:00&lt;?, ? examples/s]"}},"aab7b83a08c542a8b9c2c6a7361c6fd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"c7a6a9e6881541bc963072bdd286310b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41375e74f98b42ecb1bf856be1351b52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"accb31441c2e4946b13baf02498a3b20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246e1debf56a4f2baed2cab5bc741fd6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"362d044fd30b47a5bc9b1e29d312cbcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"720e34f67cf04964b6aa232f56799511":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7dae38cf09f44dd1b80f748c8d95f1f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c6085707ab9430082c73156345b33c7","IPY_MODEL_de19507e13c44eb497379b59be0c2456","IPY_MODEL_63743c0621c640f9a68e87ba4a5bff73"],"layout":"IPY_MODEL_d83fdde704e34e9ba46e431e51ae8059"}},"3c6085707ab9430082c73156345b33c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7e01a5f424345b392be336ab53a2ea3","placeholder":"​","style":"IPY_MODEL_d2e798cefcea4f26ab993822916a3ae6","value":"Aligning the labels:   0%"}},"de19507e13c44eb497379b59be0c2456":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a91e3061831c40e495214eb19be5fdff","max":448,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fa2ccb5a31b4ae892628f2c64b10a8b","value":448}},"63743c0621c640f9a68e87ba4a5bff73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcc8ed5d26024d0bb152e74577716be2","placeholder":"​","style":"IPY_MODEL_019b62ae29de4bcf870e2d282517077f","value":" 0/448 [00:00&lt;?, ? examples/s]"}},"d83fdde704e34e9ba46e431e51ae8059":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d7e01a5f424345b392be336ab53a2ea3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2e798cefcea4f26ab993822916a3ae6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a91e3061831c40e495214eb19be5fdff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa2ccb5a31b4ae892628f2c64b10a8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bcc8ed5d26024d0bb152e74577716be2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"019b62ae29de4bcf870e2d282517077f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}